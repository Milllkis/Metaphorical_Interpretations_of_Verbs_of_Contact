{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMsJBFZa/P+nxS/6Ahec6mN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20782,"status":"ok","timestamp":1748455384926,"user":{"displayName":"Милена","userId":"18103358405036649457"},"user_tz":-180},"id":"mhF8eR1xPHTK","outputId":"2e886457-2fd1-4cc9-c00d-5fed0c9a9cba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"1dGCo4wgeM6-","executionInfo":{"status":"ok","timestamp":1748455384929,"user_tz":-180,"elapsed":4,"user":{"displayName":"Милена","userId":"18103358405036649457"}}},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","source":["import seaborn as sns\n","\n","\n","sns.set(style=\"whitegrid\")\n","palette = sns.color_palette(\"husl\", 8)\n","ITALIAN_COLOR = sns.color_palette(\"husl\", 8)[0]\n","ENGLISH_COLOR = sns.color_palette(\"husl\", 8)[3]"],"metadata":{"id":"t1oqGbRvZTA_","executionInfo":{"status":"ok","timestamp":1748455388067,"user_tz":-180,"elapsed":3123,"user":{"displayName":"Милена","userId":"18103358405036649457"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6VNfc-3qQ6ZM"},"source":["## Imports"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":34643,"status":"ok","timestamp":1748455422712,"user":{"displayName":"Милена","userId":"18103358405036649457"},"user_tz":-180},"id":"dCEWci6T_y5B","outputId":"ff679681-7821-43b2-a219-b344a70521d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting it-core-news-sm==3.8.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-3.8.0/it_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: it-core-news-sm\n","Successfully installed it-core-news-sm-3.8.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('it_core_news_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}],"source":["!python -m spacy download it_core_news_sm"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":60408,"status":"ok","timestamp":1748455483123,"user":{"displayName":"Милена","userId":"18103358405036649457"},"user_tz":-180},"id":"4PDMcejo0app","outputId":"27e3fb68-6a2c-404d-8b13-d41b8e1897ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting en-core-web-sm==3.8.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}],"source":["!python -m spacy download en_core_web_sm"]},{"cell_type":"code","source":["!pip install transformers torch pandas scikit-learn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"7Ehh-ybNnAA4","executionInfo":{"status":"ok","timestamp":1748455696019,"user_tz":-180,"elapsed":35120,"user":{"displayName":"Милена","userId":"18103358405036649457"}},"outputId":"ddca0902-7fc0-4fd3-82b8-bc1d09d520b2"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import re\n","import spacy\n","import sys\n","\n","import torch\n","import torch.nn as nn\n","from transformers import AutoModel, AutoTokenizer\n","from transformers import AutoTokenizer, AutoModelForMaskedLM\n","import umap\n","\n","from sklearn.cluster import KMeans, AgglomerativeClustering\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import mean_squared_error, silhouette_score, adjusted_rand_score, mutual_info_score, jaccard_score, pairwise_distances\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.model_selection import ParameterGrid\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.manifold import trustworthiness\n","from scipy.stats import spearmanr\n","\n","from sklearn.cluster import DBSCAN, SpectralClustering\n","from scipy.sparse import csr_matrix\n","from scipy.cluster.hierarchy import linkage, fcluster\n","from scipy.spatial.distance import squareform\n","from concurrent.futures import ThreadPoolExecutor\n","\n","from tqdm import tqdm\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.colors as mcolors\n","from matplotlib.colors import to_hex\n","\n","from scipy.cluster.hierarchy import dendrogram, linkage\n","from scipy.spatial.distance import cdist, pdist\n","\n","from hdbscan import HDBSCAN\n","import nltk\n","from nltk.util import bigrams\n","\n","import pickle\n","\n","from transformers import AutoTokenizer, AutoModelForMaskedLM\n","from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\n","\n","from sklearn.preprocessing import normalize\n","\n","from sklearn.cluster import DBSCAN, SpectralClustering\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","from sklearn.model_selection import ParameterGrid\n","from sklearn.neighbors import NearestNeighbors\n","from scipy.spatial.distance import pdist, squareform\n","from scipy.sparse.csgraph import laplacian\n","import hdbscan\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from itertools import combinations\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","from scipy.spatial import ConvexHull\n","import matplotlib.patches as mpatches\n","from matplotlib.patches import Ellipse\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics.pairwise import pairwise_distances\n","from scipy.spatial.distance import cdist\n","\n","\n","from transformers import AutoTokenizer, AutoModelForMaskedLM\n","from sklearn.metrics import calinski_harabasz_score\n","from sklearn.metrics import silhouette_score, davies_bouldin_score\n","\n","from sklearn.cluster import DBSCAN, SpectralClustering\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","from sklearn.model_selection import ParameterGrid\n","from sklearn.neighbors import NearestNeighbors\n","from scipy.spatial.distance import pdist, squareform\n","from scipy.sparse.csgraph import laplacian\n","import hdbscan\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from itertools import combinations\n"],"metadata":{"id":"sWTTv1M_Ro1p","executionInfo":{"status":"ok","timestamp":1748463679163,"user_tz":-180,"elapsed":72,"user":{"displayName":"Милена","userId":"18103358405036649457"}}},"execution_count":45,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D3H6zWgTWRJ_"},"source":["## Verbs lists"]},{"cell_type":"markdown","source":["### Italian"],"metadata":{"id":"GOaMf4QN9BXO"}},{"cell_type":"code","execution_count":10,"metadata":{"id":"5cdYnZ3EWTU2","executionInfo":{"status":"ok","timestamp":1748455784812,"user_tz":-180,"elapsed":46,"user":{"displayName":"Милена","userId":"18103358405036649457"}}},"outputs":[],"source":["italian_verbs = ['toccare', 'sfiorare', 'colpire', 'urtare', 'sbattere', 'tamponare',\n","                'scontrarsi', 'spingere', 'spintonare', 'schiaffeggiare', 'calciare',\n","                'leccare', 'baciare', 'frustare', 'punzecchiare', 'pungere', 'schiacciare',\n","                'premere', 'cliccare', 'appoggiare', 'poggiare'\n","                 ]"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"Py2nrtE5fBey","executionInfo":{"status":"ok","timestamp":1748455784759,"user_tz":-180,"elapsed":5,"user":{"displayName":"Милена","userId":"18103358405036649457"}}},"outputs":[],"source":["italian_to_russian = {\n","    \"toccare\": \"тронуть\",\n","    \"sfiorare\": \"коснуться\",\n","    \"colpire\": \"ударить\",\n","    \"urtare\": \"удариться\",\n","    \"sbattere\": \"удариться\",\n","    \"tamponare\": \"врезаться\",\n","    \"scontrarsi\": \"столкнуться\",\n","    \"spingere\": \"толкнуть\",\n","    \"spintonare\": \"толкнуть\",\n","    \"schiaffeggiare\": \"шлёпнуть\",\n","    \"calciare\": \"пнуть\",\n","    \"leccare\": \"лизнуть\",\n","    \"baciare\": \"поцеловать\",\n","    \"frustare\": \"хлестнуть\",\n","    \"punzecchiare\": \"кольнуть\",\n","    \"pungere\": \"кольнуть\",\n","    \"schiacciare\": \"нажать\",\n","    \"premere\": \"нажать\",\n","    \"cliccare\": \"кликнуть\",\n","    \"appoggiare\": \"приложить\",\n","    \"poggiare\": \"приложить\"\n","}"]},{"cell_type":"markdown","source":["### English"],"metadata":{"id":"AZGbBcRz9D4e"}},{"cell_type":"code","execution_count":11,"metadata":{"id":"g_vMSCr4XkKN","executionInfo":{"status":"ok","timestamp":1748455784817,"user_tz":-180,"elapsed":9,"user":{"displayName":"Милена","userId":"18103358405036649457"}}},"outputs":[],"source":["english_verbs = ['touch', 'hit', 'strike', 'beat', 'bang', 'bash', 'punch', 'whack',\n","                 'kick', 'smack', 'slap', 'swat', 'pat', 'tap', 'push', 'shove',\n","                 'crash', 'bump', 'collide', 'graze', 'nick'\n","                 ]"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"1IRAmpHHQaZ-","executionInfo":{"status":"ok","timestamp":1748455784920,"user_tz":-180,"elapsed":101,"user":{"displayName":"Милена","userId":"18103358405036649457"}}},"outputs":[],"source":["english_to_russian = {\n","    \"touch\": \"тронуть\",\n","    \"hit\": \"ударить\",\n","    \"strike\": \"ударить\",\n","    \"beat\": \"ударить\",\n","    \"bang\": \"ударить\",\n","    \"bash\": \"разбить\",\n","    \"punch\": \"ударить\",\n","    \"whack\": \"шлёпнуть\",\n","    \"kick\": \"пнуть\",\n","    \"smack\": \"шлёпнуть\",\n","    \"slap\": \"шлёпнуть\",\n","    \"swat\": \"поймать\",\n","    \"pat\": \"погладить\",\n","    \"tap\": \"постучать\",\n","    \"push\": \"толкнуть\",\n","    \"shove\": \"толкнуть\",\n","    \"crash\": \"разбиться\",\n","    \"bump\": \"столкнуться\",\n","    \"collide\": \"столкнуться\",\n","    \"graze\": \"коснуться\",\n","    \"nick\": \"царапнуть\"\n","}"]},{"cell_type":"markdown","metadata":{"id":"vsc8W7AFYoSc"},"source":["## Italian Tweets"]},{"cell_type":"code","source":["italian_df = pd.read_csv(\"/content/drive/MyDrive/Диплом бакалавриат'25/final_italian_7to23.csv\")"],"metadata":{"id":"HKYBMFxi9dd0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["italian_df['found_verbs'].value_counts()"],"metadata":{"id":"3Gxb3Z6F9lqY","colab":{"base_uri":"https://localhost:8080/","height":742},"executionInfo":{"status":"ok","timestamp":1747911527208,"user_tz":-180,"elapsed":13,"user":{"displayName":"Милена","userId":"18103358405036649457"}},"outputId":"d453150d-abe9-457f-f64c-49c1c74db0dd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["found_verbs\n","toccare (тронуть)            3967\n","colpire (ударить)            3925\n","spingere (толкнуть)          2819\n","tamponare (врезаться)         651\n","cliccare (кликнуть)           499\n","appoggiare (приложить)        462\n","sbattere (удариться)          460\n","sfiorare (коснуться)          394\n","premere (нажать)              248\n","leccare (лизнуть)             239\n","baciare (поцеловать)          181\n","schiacciare (нажать)          146\n","pungere (кольнуть)            121\n","urtare (удариться)             71\n","poggiare (приложить)           43\n","schiaffeggiare (шлёпнуть)      39\n","punzecchiare (кольнуть)        16\n","frustare (хлестнуть)           13\n","spintonare (толкнуть)          12\n","calciare (пнуть)                4\n","Name: count, dtype: int64"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>count</th>\n","    </tr>\n","    <tr>\n","      <th>found_verbs</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>toccare (тронуть)</th>\n","      <td>3967</td>\n","    </tr>\n","    <tr>\n","      <th>colpire (ударить)</th>\n","      <td>3925</td>\n","    </tr>\n","    <tr>\n","      <th>spingere (толкнуть)</th>\n","      <td>2819</td>\n","    </tr>\n","    <tr>\n","      <th>tamponare (врезаться)</th>\n","      <td>651</td>\n","    </tr>\n","    <tr>\n","      <th>cliccare (кликнуть)</th>\n","      <td>499</td>\n","    </tr>\n","    <tr>\n","      <th>appoggiare (приложить)</th>\n","      <td>462</td>\n","    </tr>\n","    <tr>\n","      <th>sbattere (удариться)</th>\n","      <td>460</td>\n","    </tr>\n","    <tr>\n","      <th>sfiorare (коснуться)</th>\n","      <td>394</td>\n","    </tr>\n","    <tr>\n","      <th>premere (нажать)</th>\n","      <td>248</td>\n","    </tr>\n","    <tr>\n","      <th>leccare (лизнуть)</th>\n","      <td>239</td>\n","    </tr>\n","    <tr>\n","      <th>baciare (поцеловать)</th>\n","      <td>181</td>\n","    </tr>\n","    <tr>\n","      <th>schiacciare (нажать)</th>\n","      <td>146</td>\n","    </tr>\n","    <tr>\n","      <th>pungere (кольнуть)</th>\n","      <td>121</td>\n","    </tr>\n","    <tr>\n","      <th>urtare (удариться)</th>\n","      <td>71</td>\n","    </tr>\n","    <tr>\n","      <th>poggiare (приложить)</th>\n","      <td>43</td>\n","    </tr>\n","    <tr>\n","      <th>schiaffeggiare (шлёпнуть)</th>\n","      <td>39</td>\n","    </tr>\n","    <tr>\n","      <th>punzecchiare (кольнуть)</th>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <th>frustare (хлестнуть)</th>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>spintonare (толкнуть)</th>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>calciare (пнуть)</th>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> int64</label>"]},"metadata":{},"execution_count":99}]},{"cell_type":"code","source":["def sample_verbs(df: pd.DataFrame, threshold: int = 460) -> pd.DataFrame:\n","    \"\"\"\n","    Извлекает случайные контексты для глаголов, которые встречаются более заданного количества раз.\n","\n","    :param df: DataFrame с данными\n","    :param threshold: Минимальное количество вхождений для выбора глагола\n","    :return: DataFrame с отобранными контекстами\n","    \"\"\"\n","\n","    verb_counts = df['found_verbs'].value_counts()\n","\n","    common_verbs = verb_counts[verb_counts >= threshold].index\n","\n","    sampled_contexts_list = []\n","\n","    for verb in common_verbs:\n","        contexts = df[df['found_verbs'] == verb]\n","\n","        sampled_contexts = contexts.sample(n=min(threshold, len(contexts)), random_state=42)\n","        sampled_contexts_list.append(sampled_contexts)\n","\n","    return pd.concat(sampled_contexts_list, ignore_index=True)\n"],"metadata":{"id":"QwlCvUO8nByb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["italian_contexts = sample_verbs(italian_df)"],"metadata":{"id":"vXoNx8UqnaYh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def enumerate_verbs(df):\n","    df = df.sort_values(by='found_verbs').reset_index(drop=True)\n","\n","    df['context_id'] = df.groupby('found_verbs').cumcount() + 1\n","\n","    df['context_id'] = df['found_verbs'] + '_' + df['context_id'].astype(str)\n","\n","    return df\n","\n","italian_contexts = enumerate_verbs(italian_contexts)"],"metadata":{"id":"u517yeBVQlfl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["italian_contexts = italian_contexts[['id', 'context_id', 'text',  'context', 'found_verbs', 'spacy_word_count']]\n","italian_contexts['found_verbs'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":335},"id":"nD8r9Nqfnbvh","executionInfo":{"status":"ok","timestamp":1747914365251,"user_tz":-180,"elapsed":40,"user":{"displayName":"Милена","userId":"18103358405036649457"}},"outputId":"7d386f1b-e296-4a45-b32a-87b978d1fca6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["found_verbs\n","appoggiare (приложить)    460\n","cliccare (кликнуть)       460\n","colpire (ударить)         460\n","sbattere (удариться)      460\n","spingere (толкнуть)       460\n","tamponare (врезаться)     460\n","toccare (тронуть)         460\n","Name: count, dtype: int64"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>count</th>\n","    </tr>\n","    <tr>\n","      <th>found_verbs</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>appoggiare (приложить)</th>\n","      <td>460</td>\n","    </tr>\n","    <tr>\n","      <th>cliccare (кликнуть)</th>\n","      <td>460</td>\n","    </tr>\n","    <tr>\n","      <th>colpire (ударить)</th>\n","      <td>460</td>\n","    </tr>\n","    <tr>\n","      <th>sbattere (удариться)</th>\n","      <td>460</td>\n","    </tr>\n","    <tr>\n","      <th>spingere (толкнуть)</th>\n","      <td>460</td>\n","    </tr>\n","    <tr>\n","      <th>tamponare (врезаться)</th>\n","      <td>460</td>\n","    </tr>\n","    <tr>\n","      <th>toccare (тронуть)</th>\n","      <td>460</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> int64</label>"]},"metadata":{},"execution_count":150}]},{"cell_type":"code","source":["italian_contexts.to_csv(\"/content/drive/MyDrive/Диплом бакалавриат'25/italian_contexts.csv\")"],"metadata":{"id":"9LQqnr4Wvqd3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### UmBERTo"],"metadata":{"id":"6RIxqQ0MUeLp"}},{"cell_type":"code","source":["model_name = \"Musixmatch/umberto-commoncrawl-cased-v1\"\n","model = AutoModel.from_pretrained(model_name)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"],"metadata":{"id":"TrIezcvZqnrD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Предобработка текста"],"metadata":{"id":"NN7-BCyBouSq"}},{"cell_type":"code","source":["italian_df = pd.read_csv(\"/content/drive/MyDrive/Диплом бакалавриат'25/italian_contexts.csv\")\n","italian_df = italian_df[['id', 'context_id', 'text', 'context', 'found_verbs', 'spacy_word_count']]"],"metadata":{"id":"NfnUqh9gqU1o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_tweet(tweet):\n","    # Удаление URL\n","    tweet = re.sub(r'http\\S+', '', tweet)\n","    tweet = re.sub(r'@link', '', tweet)\n","    # Замена упоминаний\n","    tweet = re.sub(r'@\\w+', '@user', tweet)\n","    # Обработка хэштегов\n","    tweet = re.sub(r'#(\\w+)', lambda x: x.group(1), tweet)\n","    # Удаление линих пробелов\n","    tweet = re.sub(r\"\\s+\", ' ', tweet)\n","\n","    return tweet.strip()"],"metadata":{"id":"tlqTCJhhQoeJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["italian_df['cleaned_text'] = italian_df['context'].apply(preprocess_tweet)"],"metadata":{"id":"DOZgh9ANo19z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["italian_df.to_csv(\"/content/drive/MyDrive/Диплом бакалавриат'25/italian_contexts.csv\")"],"metadata":{"id":"zxQ53mqaygUL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Создание эмбеддингов"],"metadata":{"id":"y0K3eX2Tz7b9"}},{"cell_type":"code","source":["italian_df = pd.read_csv(\"/content/drive/MyDrive/Диплом бакалавриат'25/italian_contexts.csv\")\n","italian_df = italian_df[['id', 'context_id', 'text',  'context', 'cleaned_text', 'found_verbs', 'spacy_word_count']]"],"metadata":{"id":"MbJFTXSRydwo","executionInfo":{"status":"ok","timestamp":1748411740103,"user_tz":-180,"elapsed":2697,"user":{"displayName":"Милена","userId":"18103358405036649457"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["nlp = spacy.load(\"it_core_news_sm\")\n","target_verbs = np.unique([verb.split()[0] for verb in italian_df['found_verbs'].tolist()])\n","\n","def extract_verbs(df):\n","    \"\"\"Извлечение целевых глаголов из оригинального текста\"\"\"\n","    verb_records = []\n","\n","    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting verbs\"):\n","        doc = nlp(row['context'])\n","        verbs = []\n","\n","        for token in doc:\n","            if token.pos_ == \"VERB\":\n","                for target_verb in target_verbs:\n","                    # Проверка частичного/полного совпадения\n","                    if target_verb in token.lemma_ or token.lemma_ in target_verb:\n","                        verbs.append({\n","                            'raw_text': token.text,  # Оригинальная словоформа\n","                            'pattern': target_verb\n","                        })\n","                        break\n","\n","        verb_records.append(verbs)\n","\n","    df['verbs_info'] = verb_records\n","    return df"],"metadata":{"id":"MzdSX_KVqx7S","executionInfo":{"status":"ok","timestamp":1748411748121,"user_tz":-180,"elapsed":7998,"user":{"displayName":"Милена","userId":"18103358405036649457"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["italian_df = extract_verbs(italian_df)"],"metadata":{"id":"5-BlQxPjJwkZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_masked_text(df):\n","    \"\"\"Создает отдельные записи для каждого целевого глагола\"\"\"\n","\n","    new_rows = []\n","\n","    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\"):\n","        text = row['cleaned_text']\n","        verbs = row['verbs_info']\n","\n","        # Для каждого глагола создаем отдельную маску\n","        for i, verb_info in enumerate(verbs):\n","            new_row = row.copy()\n","            target_verb = verb_info['raw_text']\n","\n","            # Точная замена только конкретного глагола\n","            masked_text = re.sub(\n","                r'\\b{}\\b'.format(re.escape(target_verb)),\n","                '<mask>',\n","                text,\n","                count=1  # Заменяем только первое вхождение\n","            )\n","\n","            # Проверка успешности замены\n","            if masked_text == text:\n","                print(idx)\n","                continue  # Пропускаем неудачные замены\n","\n","            # Обновляем информацию о глаголах\n","            new_verbs = [v for j, v in enumerate(verbs) if j != i]\n","            new_row['verbs_info'] = verbs\n","            new_row['cleaned_text'] = masked_text\n","            new_row['masked_verb'] = target_verb  # Добавляем новое поле\n","\n","            new_rows.append(new_row)\n","\n","    # Создаем новый DataFrame\n","    return pd.DataFrame(new_rows).reset_index(drop=True)"],"metadata":{"id":"vPBvFIpC6Nlz","executionInfo":{"status":"ok","timestamp":1748411836990,"user_tz":-180,"elapsed":38,"user":{"displayName":"Милена","userId":"18103358405036649457"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["it_featured_df = create_masked_text(italian_df)"],"metadata":{"id":"Pksjl5wC9Kjq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["it_featured_df = it_featured_df[it_featured_df['masked_verb'].apply(len) > 0]\n","len(it_featured_df)"],"metadata":{"id":"_WK5PzdFZE9J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Эксперимент с выбором слоев и методов усреднения"],"metadata":{"id":"jB1fKouNx_LQ"}},{"cell_type":"code","source":["class UmBERToEmbeddingExperiment:\n","    \"\"\"\n","    Класс для экспериментов с различными методами извлечения эмбеддингов из UmBERTo\n","    \"\"\"\n","\n","    def __init__(self, model_name=\"Musixmatch/umberto-commoncrawl-cased-v1\"):\n","        \"\"\"Инициализация модели и токенизатора\"\"\"\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        self.model = AutoModelForMaskedLM.from_pretrained(model_name, output_hidden_states=True)\n","        self.model.eval()\n","\n","        # Загружаем spaCy для анализа POS-тегов\n","        try:\n","            self.nlp = spacy.load(\"it_core_news_sm\")\n","        except OSError:\n","            print(\"Модель it_core_news_sm не найдена.\")\n","            self.nlp = None\n","\n","        # Определяем методы извлечения эмбеддингов\n","        self.extraction_methods = {\n","            # Стандартные методы\n","            'last_4_avg': self._extract_last_4_average,\n","            'last_2_avg': self._extract_last_2_average,\n","            'weighted_top_4': self._extract_weighted_top_4,\n","\n","            # Средние слои (избегаем нижние)\n","            'middle_layers_avg': self._extract_middle_layers,\n","            'attention_weighted_middle': self._extract_attention_weighted_middle,\n","\n","            # Дополнительные методы\n","            'exponential_decay': self._extract_exponential_decay,\n","            'semantic_attention': self._extract_semantic_attention,\n","            'layer_dropout_ensemble': self._extract_layer_dropout_ensemble,\n","            'dynamic_layer_selection': self._extract_dynamic_layer_selection,\n","            'contrastive_pooling': self._extract_contrastive_pooling\n","        }\n","\n","    def _get_token_embeddings(self, text, return_attention=False):\n","        \"\"\"Базовая функция для получения скрытых состояний всех слоев\"\"\"\n","        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n","\n","        with torch.no_grad():\n","            if return_attention:\n","                outputs = self.model(**inputs, output_attentions=True)\n","                return outputs.hidden_states, outputs.attentions, inputs\n","            else:\n","                outputs = self.model(**inputs)\n","                return outputs.hidden_states, None, inputs\n","\n","    # ========== СТАНДАРТНЫЕ МЕТОДЫ ==========\n","\n","    def _extract_last_4_average(self, text):\n","        \"\"\"Усреднение последних 4 слоев\"\"\"\n","        hidden_states, _, inputs = self._get_token_embeddings(text)\n","\n","        # Берем последние 4 слоя\n","        last_4 = torch.stack(hidden_states[-4:])  # [4, batch, seq_len, hidden_dim]\n","        averaged = torch.mean(last_4, dim=0)  # [batch, seq_len, hidden_dim]\n","\n","        # Усредняем по всем токенам (исключая специальные)\n","        mask = inputs['attention_mask'].bool()\n","        pooled = self._mean_pooling(averaged, mask)\n","\n","        return pooled.squeeze().numpy()\n","\n","    def _extract_last_2_average(self, text):\n","        \"\"\"Усреднение последних 2 слоев\"\"\"\n","        hidden_states, _, inputs = self._get_token_embeddings(text)\n","\n","        last_2 = torch.stack(hidden_states[-2:])\n","        averaged = torch.mean(last_2, dim=0)\n","\n","        mask = inputs['attention_mask'].bool()\n","        pooled = self._mean_pooling(averaged, mask)\n","\n","        return pooled.squeeze().numpy()\n","\n","    def _extract_weighted_top_4(self, text):\n","        \"\"\"Взвешенное усреднение топ-4 слоев с убывающими весами\"\"\"\n","        hidden_states, _, inputs = self._get_token_embeddings(text)\n","\n","        # Веса для последних 4 слоев: [0.4, 0.3, 0.2, 0.1]\n","        weights = torch.tensor([0.1, 0.2, 0.3, 0.4]).unsqueeze(1).unsqueeze(2)\n","\n","        last_4 = torch.stack(hidden_states[-4:])\n","        weighted = torch.sum(last_4 * weights, dim=0)\n","\n","        mask = inputs['attention_mask'].bool()\n","        pooled = self._mean_pooling(weighted, mask)\n","\n","        return pooled.squeeze().numpy()\n","\n","    # ========== СРЕДНИЕ СЛОИ ==========\n","\n","    def _extract_middle_layers(self, text):\n","        \"\"\"Усреднение средних слоев (слои 6-9 из 12)\"\"\"\n","        hidden_states, _, inputs = self._get_token_embeddings(text)\n","\n","        # Берем средние слои (избегаем первые 5 и последние 3)\n","        middle_layers = torch.stack(hidden_states[6:10])  # слои 6,7,8,9\n","        averaged = torch.mean(middle_layers, dim=0)\n","\n","        mask = inputs['attention_mask'].bool()\n","        pooled = self._mean_pooling(averaged, mask)\n","\n","        return pooled.squeeze().numpy()\n","\n","    def _extract_attention_weighted_middle(self, text):\n","        \"\"\"Взвешивание средних слоев на основе attention weights\"\"\"\n","        hidden_states, attentions, inputs = self._get_token_embeddings(text, return_attention=True)\n","\n","        # Берем attention веса средних слоев\n","        middle_attentions = attentions[6:10]  # слои 6-9\n","        middle_states = torch.stack(hidden_states[6:10])\n","\n","        # Усредняем attention по головам и используем как веса\n","        attention_weights = torch.stack([att.mean(dim=1) for att in middle_attentions])  # [4, batch, seq_len, seq_len]\n","        attention_weights = attention_weights.mean(dim=-1)  # [4, batch, seq_len]\n","        attention_weights = torch.softmax(attention_weights.mean(dim=-1), dim=0)  # [4, batch]\n","\n","        # Применяем веса к состояниям\n","        weighted_states = torch.sum(middle_states * attention_weights.unsqueeze(-1).unsqueeze(-1), dim=0)\n","\n","        mask = inputs['attention_mask'].bool()\n","        pooled = self._mean_pooling(weighted_states, mask)\n","\n","        return pooled.squeeze().numpy()\n","\n","    # ========== ДОПОЛНИТЕЛЬНЫЕ МЕТОДЫ ==========\n","\n","    def _extract_exponential_decay(self, text):\n","        \"\"\"Экспоненциальное затухание весов от последних к средним слоям\"\"\"\n","        hidden_states, _, inputs = self._get_token_embeddings(text)\n","\n","        # Создаем экспоненциально убывающие веса\n","        num_layers = len(hidden_states)\n","        decay_factor = 0.8\n","        weights = torch.tensor([decay_factor ** (num_layers - i - 1) for i in range(num_layers)])\n","        weights = weights / weights.sum()  # нормализуем\n","\n","        # Исключаем первые 4 слоя (слишком низкоуровневые)\n","        relevant_states = torch.stack(hidden_states[4:])\n","        relevant_weights = weights[4:] / weights[4:].sum()\n","\n","        weighted = torch.sum(relevant_states * relevant_weights.unsqueeze(1).unsqueeze(2), dim=0)\n","\n","        mask = inputs['attention_mask'].bool()\n","        pooled = self._mean_pooling(weighted, mask)\n","\n","        return pooled.squeeze().numpy()\n","\n","    def _extract_semantic_attention(self, text):\n","        \"\"\"Адаптивное взвешивание на основе семантической важности токенов\"\"\"\n","        hidden_states, attentions, inputs = self._get_token_embeddings(text, return_attention=True)\n","\n","        # Берем верхние слои (9-12)\n","        top_layers = torch.stack(hidden_states[-4:])\n","        top_attentions = attentions[-4:]\n","\n","        # Вычисляем \"семантическую важность\" как дисперсию attention весов\n","        semantic_importance = []\n","        for att in top_attentions:\n","            # att: [batch, num_heads, seq_len, seq_len]\n","            att_variance = torch.var(att.mean(dim=1), dim=-1)  # [batch, seq_len]\n","            semantic_importance.append(att_variance.mean(dim=-1))  # [batch]\n","\n","        semantic_weights = torch.stack(semantic_importance)  # [4, batch]\n","        semantic_weights = torch.softmax(semantic_weights, dim=0)\n","\n","        weighted = torch.sum(top_layers * semantic_weights.unsqueeze(-1).unsqueeze(-1), dim=0)\n","\n","        mask = inputs['attention_mask'].bool()\n","        pooled = self._mean_pooling(weighted, mask)\n","\n","        return pooled.squeeze().numpy()\n","\n","    def _extract_layer_dropout_ensemble(self, text):\n","        \"\"\"Ансамбль с случайным исключением слоев\"\"\"\n","        hidden_states, _, inputs = self._get_token_embeddings(text)\n","\n","        # Создаем несколько ансамблей с разными комбинациями слоев\n","        ensembles = []\n","        relevant_layers = list(range(6, len(hidden_states)))  # средние и верхние слои\n","\n","        for _ in range(5):  # 5 разных ансамблей\n","            # Случайно выбираем 60-80% слоев\n","            num_select = np.random.randint(int(len(relevant_layers) * 0.6), int(len(relevant_layers) * 0.8) + 1)\n","            selected_indices = np.random.choice(relevant_layers, num_select, replace=False)\n","\n","            selected_states = torch.stack([hidden_states[i] for i in selected_indices])\n","            ensemble_avg = torch.mean(selected_states, dim=0)\n","            ensembles.append(ensemble_avg)\n","\n","        # Усредняем все ансамбли\n","        final_ensemble = torch.mean(torch.stack(ensembles), dim=0)\n","\n","        mask = inputs['attention_mask'].bool()\n","        pooled = self._mean_pooling(final_ensemble, mask)\n","\n","        return pooled.squeeze().numpy()\n","\n","    def _extract_dynamic_layer_selection(self, text):\n","        \"\"\"Динамический выбор слоев на основе косинусного сходства\"\"\"\n","        hidden_states, _, inputs = self._get_token_embeddings(text)\n","\n","        # Вычисляем попарное косинусное сходство между слоями\n","        layer_similarities = []\n","        for i in range(6, len(hidden_states) - 1):  # средние и верхние слои\n","            state1 = hidden_states[i].mean(dim=1)  # [batch, hidden_dim]\n","            state2 = hidden_states[i + 1].mean(dim=1)\n","\n","            similarity = torch.cosine_similarity(state1, state2, dim=-1)\n","            layer_similarities.append(similarity.item())\n","\n","        # Выбираем слои с наименьшим сходством (наиболее информативные переходы)\n","        similarity_scores = np.array(layer_similarities)\n","        diverse_indices = np.argsort(similarity_scores)[:4]  # топ-4 наиболее различающихся\n","        diverse_indices = [i + 6 for i in diverse_indices]  # корректируем индексы\n","\n","        selected_states = torch.stack([hidden_states[i] for i in diverse_indices])\n","        averaged = torch.mean(selected_states, dim=0)\n","\n","        mask = inputs['attention_mask'].bool()\n","        pooled = self._mean_pooling(averaged, mask)\n","\n","        return pooled.squeeze().numpy()\n","\n","    def _extract_contrastive_pooling(self, text):\n","        \"\"\"Контрастивное объединение: подчеркиваем различия между слоями\"\"\"\n","        hidden_states, _, inputs = self._get_token_embeddings(text)\n","\n","        # Берем верхние слои\n","        top_layers = hidden_states[-4:]\n","\n","        # Вычисляем \"контрастивные\" веса\n","        contrasts = []\n","        for i, layer in enumerate(top_layers):\n","            if i == 0:\n","                contrasts.append(layer)\n","            else:\n","                # Подчеркиваем различия с предыдущим слоем\n","                contrast = layer - 0.3 * top_layers[i-1]\n","                contrasts.append(contrast)\n","\n","        # Комбинируем контрастивные представления\n","        final_contrast = torch.mean(torch.stack(contrasts), dim=0)\n","\n","        mask = inputs['attention_mask'].bool()\n","        pooled = self._mean_pooling(final_contrast, mask)\n","\n","        return pooled.squeeze().numpy()\n","\n","    def _mean_pooling(self, token_embeddings, attention_mask):\n","        \"\"\"Усреднение токенов с учетом маски внимания\"\"\"\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","        pooled = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","\n","        # Убеждаемся, что возвращаем 2D тензор [batch_size, hidden_dim]\n","        if pooled.dim() == 1:\n","            pooled = pooled.unsqueeze(0)\n","        elif pooled.dim() > 2:\n","            pooled = pooled.view(pooled.size(0), -1)\n","\n","        return pooled\n","\n","    def extract_embeddings_for_method(self, texts, method_name):\n","        \"\"\"Извлечение эмбеддингов для всех текстов с выбранным методом\"\"\"\n","        method = self.extraction_methods[method_name]\n","        embeddings = []\n","\n","        print(f\"Извлечение эмбеддингов методом '{method_name}'...\")\n","        for text in tqdm(texts, desc=f\"Обработка {method_name}\",\n","                        leave=True, ncols=100, file=sys.stdout):\n","            try:\n","                embedding = method(text)\n","\n","                # Проверяем и исправляем размерность\n","                if isinstance(embedding, np.ndarray):\n","                    if embedding.ndim == 1:\n","                        embeddings.append(embedding)\n","                    elif embedding.ndim == 2:\n","                        # Если 2D, берем первую строку (batch_size=1)\n","                        embeddings.append(embedding[0])\n","                    else:\n","                        # Если больше 2D, сглаживаем\n","                        embeddings.append(embedding.flatten()[:768])  # обрезаем до нужной размерности\n","                else:\n","                    # Если не numpy array, конвертируем\n","                    embedding = np.array(embedding)\n","                    if embedding.ndim > 1:\n","                        embedding = embedding.flatten()[:768]\n","                    embeddings.append(embedding)\n","\n","            except Exception as e:\n","                print(f\"Ошибка для текста: {text[:50]}... - {e}\")\n","                # Добавляем нулевой вектор в случае ошибки\n","                embeddings.append(np.zeros(768))  # размерность UmBERTo\n","\n","        # Финальная проверка размерности\n","        embeddings_array = np.array(embeddings)\n","\n","        # Если все еще есть проблемы с размерностью, приводим к правильной форме\n","        if embeddings_array.ndim != 2:\n","            print(f\"Исправление размерности массива: {embeddings_array.shape} -> (?, 768)\")\n","            embeddings_list = []\n","            for emb in embeddings:\n","                if isinstance(emb, np.ndarray):\n","                    if emb.ndim == 0:\n","                        embeddings_list.append(np.zeros(768))\n","                    elif emb.ndim == 1:\n","                        if len(emb) >= 768:\n","                            embeddings_list.append(emb[:768])\n","                        else:\n","                            padded = np.zeros(768)\n","                            padded[:len(emb)] = emb\n","                            embeddings_list.append(padded)\n","                    else:\n","                        flattened = emb.flatten()\n","                        if len(flattened) >= 768:\n","                            embeddings_list.append(flattened[:768])\n","                        else:\n","                            padded = np.zeros(768)\n","                            padded[:len(flattened)] = flattened\n","                            embeddings_list.append(padded)\n","                else:\n","                    embeddings_list.append(np.zeros(768))\n","            embeddings_array = np.array(embeddings_list)\n","\n","        print(f\"Итоговая размерность эмбеддингов: {embeddings_array.shape}\")\n","        return embeddings_array\n","\n","    def evaluate_clustering_quality(self, embeddings, labels=None):\n","        \"\"\"Оценка качества кластеризации\"\"\"\n","\n","        # Проверяем размерность входных данных\n","        print(f\"Проверка размерности эмбеддингов: {embeddings.shape}\")\n","\n","        if embeddings.ndim != 2:\n","            print(f\"Неправильная размерность эмбеддингов: {embeddings.shape}\")\n","            return {\n","                'silhouette_score': -1,\n","                'davies_bouldin_score': float('inf'),\n","                'calinski_harabasz_score': 0,\n","                'trustworthiness': 0,\n","                'n_clusters': 0,\n","                'noise_ratio': 1.0,\n","                'umap_embeddings': np.zeros((len(embeddings), 2)),\n","                'cluster_labels': np.full(len(embeddings), -1)\n","            }\n","\n","        # Проверяем на NaN и inf\n","        if np.any(np.isnan(embeddings)) or np.any(np.isinf(embeddings)):\n","            print(\"Найдены NaN или inf значения в эмбеддингах\")\n","            embeddings = np.nan_to_num(embeddings, nan=0.0, posinf=1.0, neginf=-1.0)\n","\n","        try:\n","            # Применяем UMAP для снижения размерности\n","            umap_reducer = umap.UMAP(\n","                n_components=2,\n","                n_neighbors=min(20, len(embeddings)-1),  # учитываем размер данных\n","                min_dist=0.1,\n","                metric='cosine',\n","                random_state=42\n","            )\n","\n","            umap_embeddings = umap_reducer.fit_transform(embeddings)\n","\n","            # Кластеризация с HDBSCAN\n","            min_cluster_size = max(3, min(5, len(embeddings)//10))  # адаптивный размер\n","            clusterer = HDBSCAN(min_cluster_size=min_cluster_size, metric='euclidean')\n","            cluster_labels = clusterer.fit_predict(umap_embeddings)\n","\n","        except Exception as e:\n","            print(f\"Ошибка при UMAP/кластеризации: {e}\")\n","            return {\n","                'silhouette_score': -1,\n","                'davies_bouldin_score': float('inf'),\n","                'calinski_harabasz_score': 0,\n","                'trustworthiness': 0,\n","                'n_clusters': 0,\n","                'noise_ratio': 1.0,\n","                'umap_embeddings': np.zeros((len(embeddings), 2)),\n","                'cluster_labels': np.full(len(embeddings), -1)\n","            }\n","\n","        # Исключаем шум (-1 метки) для вычисления метрик\n","        valid_mask = cluster_labels != -1\n","        if valid_mask.sum() < 2:\n","            return {\n","                'silhouette_score': -1,\n","                'davies_bouldin_score': float('inf'),\n","                'calinski_harabasz_score': 0,\n","                'trustworthiness': 0,\n","                'n_clusters': 0,\n","                'noise_ratio': 1.0,\n","                'umap_embeddings': umap_embeddings,\n","                'cluster_labels': cluster_labels\n","            }\n","\n","        valid_embeddings = umap_embeddings[valid_mask]\n","        valid_labels = cluster_labels[valid_mask]\n","\n","        # Вычисляем метрики\n","        try:\n","            silhouette = silhouette_score(valid_embeddings, valid_labels)\n","            davies_bouldin = davies_bouldin_score(valid_embeddings, valid_labels)\n","            calinski_harabasz = calinski_harabasz_score(valid_embeddings, valid_labels)\n","        except Exception as e:\n","            print(f\"Ошибка при вычислении метрик: {e}\")\n","            silhouette = -1\n","            davies_bouldin = float('inf')\n","            calinski_harabasz = 0\n","\n","        # Вычисляем trustworthiness\n","        try:\n","            from sklearn.manifold import trustworthiness as sklearn_trustworthiness\n","            trustworthiness = sklearn_trustworthiness(embeddings, umap_embeddings, n_neighbors=min(10, len(embeddings)-1))\n","        except Exception as e:\n","            print(f\"Ошибка при вычислении trustworthiness: {e}\")\n","            trustworthiness = 0\n","\n","        return {\n","            'silhouette_score': silhouette,\n","            'davies_bouldin_score': davies_bouldin,\n","            'calinski_harabasz_score': calinski_harabasz,\n","            'trustworthiness': trustworthiness,\n","            'n_clusters': len(np.unique(valid_labels)),\n","            'noise_ratio': (cluster_labels == -1).mean(),\n","            'umap_embeddings': umap_embeddings,\n","            'cluster_labels': cluster_labels\n","        }\n","\n","    def _calculate_trustworthiness(self, X_original, X_embedded, k=10):\n","        \"\"\"Вычисление trustworthiness score\"\"\"\n","        from sklearn.neighbors import NearestNeighbors\n","\n","        # Находим ближайших соседей в исходном и проецированном пространстве\n","        nn_original = NearestNeighbors(n_neighbors=k+1, metric='cosine').fit(X_original)\n","        nn_embedded = NearestNeighbors(n_neighbors=k+1, metric='euclidean').fit(X_embedded)\n","\n","        _, indices_original = nn_original.kneighbors(X_original)\n","        _, indices_embedded = nn_embedded.kneighbors(X_embedded)\n","\n","        n = X_original.shape[0]\n","        trustworthiness = 0\n","\n","        for i in range(n):\n","            # Исключаем сам элемент (первый в списке соседей)\n","            neighbors_original = set(indices_original[i][1:])\n","            neighbors_embedded = set(indices_embedded[i][1:])\n","\n","            # Находим \"интрудеров\" - точки, которые близки в проекции, но далеки в оригинале\n","            intruders = neighbors_embedded - neighbors_original\n","\n","            rank_sum = 0\n","            for intruder in intruders:\n","                # Находим ранг интрудера в исходном пространстве\n","                rank = np.where(indices_original[i] == intruder)[0]\n","                if len(rank) > 0:\n","                    rank_sum += max(0, rank[0] - k)\n","\n","            trustworthiness += rank_sum\n","\n","        # Нормализация\n","        max_sum = k * (2 * n - 3 * k - 1) / 2\n","        trustworthiness = 1 - (2 * trustworthiness) / (n * max_sum) if max_sum > 0 else 1\n","\n","        return max(0, trustworthiness)\n","\n","    def run_comprehensive_experiment(self, df, text_column='cleaned_text', verb_column='masked_verb'):\n","        \"\"\"Запуск полного эксперимента со всеми методами\"\"\"\n","        texts = df[text_column].tolist()\n","        verbs = df[verb_column].tolist() if verb_column in df.columns else None\n","\n","        print(f\"Начинаем эксперимент с {len(texts)} текстами и {len(self.extraction_methods)} методами\")\n","        print(f\"Методы: {list(self.extraction_methods.keys())}\")\n","\n","        results = {}\n","        embeddings_cache = {}\n","\n","        # Извлекаем эмбеддинги для каждого метода\n","        for method_name in tqdm(self.extraction_methods.keys(),\n","                               desc=\"Обработка методов\", leave=True, ncols=100):\n","            embeddings = self.extract_embeddings_for_method(texts, method_name)\n","            embeddings_cache[method_name] = embeddings\n","\n","            # Оцениваем качество кластеризации\n","            print(f\"Оценка качества для метода '{method_name}'...\")\n","            quality_metrics = self.evaluate_clustering_quality(embeddings, verbs)\n","\n","            results[method_name] = {\n","                'embeddings': embeddings,\n","                'metrics': quality_metrics\n","            }\n","\n","            print(f\"{method_name}: Silhouette={quality_metrics['silhouette_score']:.3f}, \"\n","                  f\"Clusters={quality_metrics['n_clusters']}, \"\n","                  f\"Trustworthiness={quality_metrics['trustworthiness']:.3f}\")\n","\n","        return results\n","\n","    def plot_comparison_results(self, results, save_path=None):\n","          \"\"\"\n","          Визуализация результатов сравнения методов извлечения эмбеддингов.\n","\n","          Args:\n","              results (dict): Результаты выполнения экспериментов\n","              save_path (str, optional): Путь для сохранения графиков\n","\n","          Returns:\n","              list: Ранжированный список методов по композитному рейтингу\n","          \"\"\"\n","          # Настройка стиля графиков\n","          sns.set(style=\"whitegrid\")\n","          palette = sns.color_palette(\"husl\", 8)\n","          plt.figure(figsize=(14, 10))\n","\n","\n","    def plot_comparison_results(self, results, save_path=None):\n","          \"\"\"Визуализация результатов сравнения методов\"\"\"\n","          # Подготавливаем данные для сравнения\n","          methods = list(results.keys())\n","          metrics_data = {\n","              'Method': [],\n","              'Silhouette Score': [],\n","              'Davies-Bouldin Score': [],\n","              'Calinski-Harabasz Score': [],\n","              'Trustworthiness': [],\n","              'N Clusters': [],\n","              'Noise Ratio': []\n","          }\n","\n","          for method in methods:\n","              metrics = results[method]['metrics']\n","              metrics_data['Method'].append(method)\n","              metrics_data['Silhouette Score'].append(metrics['silhouette_score'])\n","              metrics_data['Davies-Bouldin Score'].append(metrics['davies_bouldin_score'])\n","              metrics_data['Calinski-Harabasz Score'].append(metrics['calinski_harabasz_score'])\n","              metrics_data['Trustworthiness'].append(metrics['trustworthiness'])\n","              metrics_data['N Clusters'].append(metrics['n_clusters'])\n","              metrics_data['Noise Ratio'].append(metrics['noise_ratio'])\n","\n","          # Создаем большую фигуру для всех графиков\n","          fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n","          fig.suptitle('Сравнение методов извлечения эмбеддингов UmBERTo', fontsize=16, fontweight='bold')\n","\n","          # График 1: Silhouette Score\n","          ax1 = axes[0, 0]\n","          bars1 = ax1.bar(range(len(methods)), metrics_data['Silhouette Score'],\n","                        color=palette[:len(methods)])\n","          ax1.set_title('Silhouette Score (выше = лучше)')\n","          ax1.set_xlabel('Методы')\n","          ax1.set_ylabel('Score')\n","          ax1.set_xticks(range(len(methods)))\n","          ax1.set_xticklabels(methods, rotation=45, ha='right')\n","          ax1.grid(True, alpha=0.3)\n","\n","          # График 2: Davies-Bouldin Score\n","          ax2 = axes[0, 1]\n","          bars2 = ax2.bar(range(len(methods)), metrics_data['Davies-Bouldin Score'],\n","                        color=palette[:len(methods)])\n","          ax2.set_title('Davies-Bouldin Score (ниже = лучше)')\n","          ax2.set_xlabel('Методы')\n","          ax2.set_ylabel('Score')\n","          ax2.set_xticks(range(len(methods)))\n","          ax2.set_xticklabels(methods, rotation=45, ha='right')\n","          ax2.grid(True, alpha=0.3)\n","\n","          # График 3: Trustworthiness\n","          ax3 = axes[0, 2]\n","          bars3 = ax3.bar(range(len(methods)), metrics_data['Trustworthiness'],\n","                        color=palette[:len(methods)])\n","          ax3.set_title('Trustworthiness (выше = лучше)')\n","          ax3.set_xlabel('Методы')\n","          ax3.set_ylabel('Score')\n","          ax3.set_xticks(range(len(methods)))\n","          ax3.set_xticklabels(methods, rotation=45, ha='right')\n","          ax3.grid(True, alpha=0.3)\n","\n","          # График 4: Количество кластеров\n","          ax4 = axes[1, 0]\n","          bars4 = ax4.bar(range(len(methods)), metrics_data['N Clusters'],\n","                        color=palette[:len(methods)])\n","          ax4.set_title('Количество кластеров')\n","          ax4.set_xlabel('Методы')\n","          ax4.set_ylabel('Количество')\n","          ax4.set_xticks(range(len(methods)))\n","          ax4.set_xticklabels(methods, rotation=45, ha='right')\n","          ax4.grid(True, alpha=0.3)\n","\n","          # График 5: Доля шума\n","          ax5 = axes[1, 1]\n","          bars5 = ax5.bar(range(len(methods)), metrics_data['Noise Ratio'],\n","                        color=palette[:len(methods)])\n","          ax5.set_title('Доля шума (ниже = лучше)')\n","          ax5.set_xlabel('Методы')\n","          ax5.set_ylabel('Доля')\n","          ax5.set_xticks(range(len(methods)))\n","          ax5.set_xticklabels(methods, rotation=45, ha='right')\n","          ax5.grid(True, alpha=0.3)\n","\n","          # График 6: Комбинированный рейтинг\n","          ax6 = axes[1, 2]\n","          # Нормализуем метрики и создаем составной рейтинг\n","          normalized_silhouette = np.array(metrics_data['Silhouette Score'])\n","          normalized_trustworthiness = np.array(metrics_data['Trustworthiness'])\n","          normalized_db = 1 / (1 + np.array(metrics_data['Davies-Bouldin Score']))  # инвертируем\n","          normalized_noise = 1 - np.array(metrics_data['Noise Ratio'])  # инвертируем\n","\n","          composite_score = (normalized_silhouette + normalized_trustworthiness +\n","                            normalized_db + normalized_noise) / 4\n","\n","          bars6 = ax6.bar(range(len(methods)), composite_score,\n","                        color=palette[:len(methods)])\n","          ax6.set_title('Композитный рейтинг (выше = лучше)')\n","          ax6.set_xlabel('Методы')\n","          ax6.set_ylabel('Score')\n","          ax6.set_xticks(range(len(methods)))\n","          ax6.set_xticklabels(methods, rotation=45, ha='right')\n","          ax6.grid(True, alpha=0.3)\n","\n","          plt.tight_layout()\n","\n","          if save_path:\n","              plt.savefig(save_path, dpi=300, bbox_inches='tight')\n","\n","          plt.show()\n","\n","          # Возвращаем рейтинг методов\n","          ranking = sorted(zip(methods, composite_score), key=lambda x: x[1], reverse=True)\n","          return ranking"],"metadata":{"id":"CtHj5fmX5gFQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["experiment = UmBERToEmbeddingExperiment()\n","\n","# Запуск полного эксперимента\n","results = experiment.run_comprehensive_experiment(\n","    it_featured_df[:200],\n","    text_column='cleaned_text',    # колонка с маскированными текстами\n","    verb_column='masked_verb'      # колонка с замаскированными глаголами\n",")\n","\n","# Визуализация результатов\n","ranking = experiment.plot_comparison_results(results)\n","\n","# Вывод лучших методов\n","print(\"ТОП-3 метода:\")\n","for i, (method, score) in enumerate(ranking[:3], 1):\n","    print(f\"{i}. {method}: {score:.4f}\")"],"metadata":{"id":"G4zHPOK1Ca-T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Cоздание эмбеддингов с помощью лучшего метода агрегации"],"metadata":{"id":"mj1hII4evOvY"}},{"cell_type":"code","source":["def generate_weighted_top4_embeddings(\n","    texts,\n","    model,\n","    tokenizer,\n","    layer_weights=[0.1, 0.2, 0.3, 0.4],\n","    max_length=512,\n","    device=\"cpu\",\n","    batch_size=16,\n","    show_progress=True\n","):\n","    \"\"\"\n","    Генерирует эмбеддинги с прогресс-баром и пакетной обработкой\n","\n","    Параметры:\n","        batch_size (int): Размер пакета для обработки\n","        show_progress (bool): Показывать прогресс-бар\n","    \"\"\"\n","    # Валидация входных данных\n","    if not texts:\n","        return np.array([])\n","\n","    if len(layer_weights) != 4:\n","        raise ValueError(\"layer_weights должен содержать 4 элемента\")\n","\n","    # Подготовка прогресс-бара\n","    total_batches = (len(texts) + batch_size - 1) // batch_size\n","    pbar = tqdm(total=total_batches, desc=\"Generating embeddings\",\n","                disable=not show_progress)\n","\n","    all_embeddings = []\n","\n","    # Обработка батчами\n","    for i in range(0, len(texts), batch_size):\n","        batch_texts = texts[i:i+batch_size]\n","\n","        # Токенизация батча\n","        inputs = tokenizer(\n","            batch_texts,\n","            return_tensors=\"pt\",\n","            padding=True,\n","            truncation=True,\n","            max_length=max_length\n","        ).to(device)\n","\n","        # Получение эмбеддингов\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","            hidden_states = outputs.hidden_states\n","\n","        # Взвешенное объединение слоев\n","        last_4 = torch.stack(hidden_states[-4:])\n","        weights = torch.tensor(layer_weights, device=device).view(4, 1, 1, 1)\n","        weighted = (last_4 * weights).sum(dim=0)\n","\n","        # Пулинг с маской\n","        mask = inputs['attention_mask'].unsqueeze(-1).float()\n","        summed = torch.sum(weighted * mask, dim=1)\n","        counts = torch.clamp(mask.sum(dim=1), min=1e-9)\n","        batch_embeddings = (summed / counts).cpu().numpy()\n","\n","        all_embeddings.append(batch_embeddings)\n","        pbar.update(1)\n","\n","    pbar.close()\n","    return np.vstack(all_embeddings)"],"metadata":{"id":"277Ojar_JfM7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = \"Musixmatch/umberto-commoncrawl-cased-v1\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForMaskedLM.from_pretrained(model_name, output_hidden_states=True)\n","model.eval()"],"metadata":{"collapsed":true,"id":"mp3N3EjAzTPC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embeddings = generate_weighted_top4_embeddings(\n","    texts=it_featured_df['cleaned_text'].to_list(),\n","    model=model,\n","    tokenizer=tokenizer,\n","    layer_weights=[0.1, 0.2, 0.3, 0.4],\n","    max_length=512,\n","    device=\"cpu\"\n",")\n","\n","print(f\"Размерность эмбеддингов: {embeddings.shape}\")\n","print(f\"Пример эмбеддинга: {embeddings[0][:10]}\")"],"metadata":{"id":"f8iTplW-n8oB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","with open(\"/content/drive/MyDrive/Диплом бакалавриат'25/italian_embeddings.pkl\", 'wb') as f:\n","    pickle.dump(embeddings, f)"],"metadata":{"id":"ZPsKhySrvx5p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(\"/content/drive/MyDrive/Диплом бакалавриат'25/italian_embeddings.pkl\", 'rb') as f:\n","    embeddings = pickle.load(f)"],"metadata":{"id":"sa9-S9riqa1T","executionInfo":{"status":"ok","timestamp":1748411842988,"user_tz":-180,"elapsed":664,"user":{"displayName":"Милена","userId":"18103358405036649457"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["embeddings_np = np.array(embeddings)"],"metadata":{"id":"otmAWoToqEDr","executionInfo":{"status":"ok","timestamp":1748411842999,"user_tz":-180,"elapsed":5,"user":{"displayName":"Милена","userId":"18103358405036649457"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# Проверка нормализации\n","norms = np.linalg.norm(embeddings_np, axis=1)\n","print(\"Минимальная норма:\", np.min(norms))\n","print(\"Максимальная норма:\", np.max(norms))"],"metadata":{"id":"DCQWdTAexuCL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Проверка близости к 1\n","tolerance = 1e-5\n","is_normalized = np.allclose(norms, 1.0, atol=tolerance)\n","print(f\"Все эмбеддинги нормализованы (с погрешностью {tolerance})? {is_normalized}\")"],"metadata":{"id":"Hal_DBIlxw8Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embeddings_l2 = normalize(embeddings, norm='l2')"],"metadata":{"id":"iendmv3Tx0zR","executionInfo":{"status":"ok","timestamp":1748411843070,"user_tz":-180,"elapsed":18,"user":{"displayName":"Милена","userId":"18103358405036649457"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["#### Снижение размерности"],"metadata":{"id":"_PPoJ7mlp3wN"}},{"cell_type":"code","source":["param_grid = {\n","    'n_neighbors': np.arange(10, 50, 10),\n","    'min_dist': [0.01, 0.05],\n","    'n_components': np.arange(2, 30, 5),\n","    'metric': ['cosine', 'correlation']\n","}"],"metadata":{"id":"L5VNcvTZvUdV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def bootstrap_stability(embeddings, params, n_samples=5, sample_size=0.8):\n","    stability = []\n","    for _ in range(n_samples):\n","        idx = np.random.choice(len(embeddings), int(len(embeddings)*sample_size))\n","        proj1 = umap.UMAP(**params).fit_transform(embeddings[idx])\n","        proj2 = umap.UMAP(**params).fit_transform(embeddings[idx])\n","        stability.append(spearmanr(proj1.flatten(), proj2.flatten())[0])\n","    return np.mean(stability)"],"metadata":{"id":"nV9xuNy2vTg7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def optimize_umap_params(embeddings, param_grid, sample_size=1000, random_state=42):\n","    \"\"\"\n","    Оптимизирует параметры UMAP для заданных эмбеддингов с использованием грид-поиска.\n","\n","    Параметры:\n","    embeddings (np.array/torch.Tensor): Массив эмбеддингов\n","    param_grid (dict): Сетка параметров для поиска\n","    sample_size (int): Размер выборки для ускорения вычислений\n","    random_state (int): Seed для воспроизводимости\n","\n","    Возвращает:\n","    pd.DataFrame: Результаты экспериментов с метриками\n","    \"\"\"\n","    # Сэмплирование данных\n","    embeddings = embeddings[:sample_size]\n","\n","    # Стандартизация\n","    scaler = StandardScaler()\n","    embeddings_scaled = scaler.fit_transform(embeddings)\n","\n","    # Основной цикл оптимизации\n","    results = []\n","    for params in tqdm(ParameterGrid(param_grid), desc=\"Grid Search\"):\n","        reducer = umap.UMAP(**params, random_state=random_state)\n","        projection = reducer.fit_transform(embeddings_scaled)\n","\n","        # Вычисление метрик\n","        d_orig = pdist(embeddings_scaled, metric=params['metric'])\n","        d_proj = pdist(projection, metric='euclidean')\n","\n","        metrics = {\n","            'trustworthiness': trustworthiness(embeddings_scaled, projection,\n","                                             n_neighbors=params['n_neighbors']),\n","            'stability': bootstrap_stability(embeddings_scaled, params),\n","            'global_spearman': spearmanr(d_orig, d_proj).correlation\n","        }\n","\n","        results.append({**params, **metrics})\n","\n","    return pd.DataFrame(results)\n"],"metadata":{"id":"Pr8Rul00vw1G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_df = optimize_umap_params(\n","    embeddings=embeddings_l2,\n","    param_grid=param_grid,\n","    sample_size=460\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m48IWloexgcu","executionInfo":{"status":"ok","timestamp":1748353428091,"user_tz":-180,"elapsed":1522094,"user":{"displayName":"Милена","userId":"18103358405036649457"}},"outputId":"50d5d923-db0c-4268-942e-a43a4fe5b919"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Grid Search: 100%|██████████| 96/96 [25:22<00:00, 15.86s/it]\n"]}]},{"cell_type":"code","source":["def find_best_parameters(results_df, metric_weights=None):\n","    \"\"\"\n","    Находит оптимальные параметры на основе взвешенной комбинации метрик.\n","\n","    Параметры:\n","    results_df (pd.DataFrame): Результаты экспериментов\n","    metric_weights (dict): Веса для метрик (по умолчанию равные веса)\n","\n","    Возвращает:\n","    pd.DataFrame: Топ-5 лучших комбинаций с нормализованными оценками\n","    \"\"\"\n","    df = results_df.copy()\n","\n","    # Задаем веса по умолчанию (равные веса)\n","    if not metric_weights:\n","        metric_weights = {\n","            'trustworthiness': 0.5,\n","            'stability': 0.3,\n","            'global_spearman': 0.2\n","        }\n","\n","    # Нормализуем метрики (Min-Max scaling)\n","    for metric in metric_weights:\n","        df[metric + '_norm'] = (df[metric] - df[metric].min()) / (df[metric].max() - df[metric].min())\n","\n","    # Рассчитываем общий score\n","    df['total_score'] = sum(\n","        df[metric + '_norm'] * weight\n","        for metric, weight in metric_weights.items()\n","    )\n","\n","    # Сортируем по убыванию общего score\n","    best_results = df.sort_values('total_score', ascending=False).head(5)\n","\n","    return best_results\n","\n","best_df = find_best_parameters(results_df)\n","\n","best_df[['n_neighbors', 'min_dist', 'n_components', 'metric',\n","               'trustworthiness', 'stability', 'global_spearman', 'total_score']]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"6lH9vO5WsQbd","executionInfo":{"status":"ok","timestamp":1748353428143,"user_tz":-180,"elapsed":58,"user":{"displayName":"Милена","userId":"18103358405036649457"}},"outputId":"383f9f8e-dda3-499e-a3f0-ac384a2f3c69"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    n_neighbors  min_dist  n_components       metric  trustworthiness  \\\n","44           10      0.05            27       cosine         0.869044   \n","84           10      0.05            17  correlation         0.871882   \n","64           10      0.01            22  correlation         0.870132   \n","88           10      0.05            22  correlation         0.872342   \n","36           10      0.05            17       cosine         0.870148   \n","\n","    stability  global_spearman  total_score  \n","44   0.841935         0.485025     0.817485  \n","84   0.811681         0.481386     0.816966  \n","64   0.862210         0.470050     0.809362  \n","88   0.790485         0.482010     0.809104  \n","36   0.844489         0.473403     0.805252  "],"text/html":["\n","  <div id=\"df-c227e857-8264-4278-be87-625eeb17748d\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>n_neighbors</th>\n","      <th>min_dist</th>\n","      <th>n_components</th>\n","      <th>metric</th>\n","      <th>trustworthiness</th>\n","      <th>stability</th>\n","      <th>global_spearman</th>\n","      <th>total_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>44</th>\n","      <td>10</td>\n","      <td>0.05</td>\n","      <td>27</td>\n","      <td>cosine</td>\n","      <td>0.869044</td>\n","      <td>0.841935</td>\n","      <td>0.485025</td>\n","      <td>0.817485</td>\n","    </tr>\n","    <tr>\n","      <th>84</th>\n","      <td>10</td>\n","      <td>0.05</td>\n","      <td>17</td>\n","      <td>correlation</td>\n","      <td>0.871882</td>\n","      <td>0.811681</td>\n","      <td>0.481386</td>\n","      <td>0.816966</td>\n","    </tr>\n","    <tr>\n","      <th>64</th>\n","      <td>10</td>\n","      <td>0.01</td>\n","      <td>22</td>\n","      <td>correlation</td>\n","      <td>0.870132</td>\n","      <td>0.862210</td>\n","      <td>0.470050</td>\n","      <td>0.809362</td>\n","    </tr>\n","    <tr>\n","      <th>88</th>\n","      <td>10</td>\n","      <td>0.05</td>\n","      <td>22</td>\n","      <td>correlation</td>\n","      <td>0.872342</td>\n","      <td>0.790485</td>\n","      <td>0.482010</td>\n","      <td>0.809104</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>10</td>\n","      <td>0.05</td>\n","      <td>17</td>\n","      <td>cosine</td>\n","      <td>0.870148</td>\n","      <td>0.844489</td>\n","      <td>0.473403</td>\n","      <td>0.805252</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c227e857-8264-4278-be87-625eeb17748d')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-c227e857-8264-4278-be87-625eeb17748d button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-c227e857-8264-4278-be87-625eeb17748d');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-d04dc2a2-6ca1-47c5-a752-9025ad53296a\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d04dc2a2-6ca1-47c5-a752-9025ad53296a')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-d04dc2a2-6ca1-47c5-a752-9025ad53296a button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"               'trustworthiness', 'stability', 'global_spearman', 'total_score']]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"n_neighbors\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 10,\n        \"max\": 10,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"min_dist\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.01788854381999832,\n        \"min\": 0.01,\n        \"max\": 0.05,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.01\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_components\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 17,\n        \"max\": 27,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          27\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"metric\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"correlation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trustworthiness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.001365881715428945,\n        \"min\": 0.8690443585856116,\n        \"max\": 0.8723421528830635,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.8718824277400108\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stability\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.028665123748703843,\n        \"min\": 0.7904847230297456,\n        \"max\": 0.8622097945063792,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.8116809586972303\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"global_spearman\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.006335019548377376,\n        \"min\": 0.47004952782197557,\n        \"max\": 0.48502473026305126,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.4813860302995285\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.005360873961050443,\n        \"min\": 0.8052517634035196,\n        \"max\": 0.8174847578979515,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.8169661457265558\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["param_grid = {\n","    'n_neighbors': [10],\n","    'min_dist': [0.05],\n","    'n_components': [27],\n","    'metric': ['cosine']\n","}"],"metadata":{"id":"aRtS4iO9yKOg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_df_1 = optimize_umap_params(\n","    embeddings=embeddings_l2,\n","    param_grid=param_grid,\n","    sample_size=len(embeddings_np)\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748353696901,"user_tz":-180,"elapsed":243291,"user":{"displayName":"Милена","userId":"18103358405036649457"}},"outputId":"74ce5934-004a-4aa9-e128-2c31d33ab25d","id":"ZoNcjNyXyKOh"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Grid Search: 100%|██████████| 1/1 [04:03<00:00, 243.13s/it]\n"]}]},{"cell_type":"code","source":["param_grid = {\n","    'n_neighbors': [10],\n","    'min_dist': [0.05],\n","    'n_components': [17],\n","    'metric': ['correlation']\n","}"],"metadata":{"id":"4hXY90odtf1m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_df_2 = optimize_umap_params(\n","    embeddings=embeddings_l2,\n","    param_grid=param_grid,\n","    sample_size=len(embeddings_np)\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748353954414,"user_tz":-180,"elapsed":257517,"user":{"displayName":"Милена","userId":"18103358405036649457"}},"outputId":"4ab008aa-7175-4b3f-c82b-20102177382e","id":"cgtn_w95tf1n"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Grid Search: 100%|██████████| 1/1 [04:17<00:00, 257.43s/it]\n"]}]},{"cell_type":"code","source":["param_grid = {\n","    'n_neighbors': [10],\n","    'min_dist': [0.01],\n","    'n_components': [22],\n","    'metric': ['correlation']\n","}"],"metadata":{"id":"ZoVcXq6cyYCc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_df_3 = optimize_umap_params(\n","    embeddings=embeddings_l2,\n","    param_grid=param_grid,\n","    sample_size=len(embeddings_np)\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748354223161,"user_tz":-180,"elapsed":268751,"user":{"displayName":"Милена","userId":"18103358405036649457"}},"outputId":"28b173a0-9183-4140-882f-fb366dd998f6","id":"JBnlju7oyYCc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Grid Search: 100%|██████████| 1/1 [04:28<00:00, 268.72s/it]\n"]}]},{"cell_type":"code","source":["results_best = pd.concat([results_df_1, results_df_2, results_df_3], axis=0)\n","results_best"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"vf9pWISxyfo4","executionInfo":{"status":"ok","timestamp":1748354223165,"user_tz":-180,"elapsed":29,"user":{"displayName":"Милена","userId":"18103358405036649457"}},"outputId":"2ce953e9-f5b7-4b70-ac41-3b7c0f86dcd3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        metric  min_dist  n_components  n_neighbors  trustworthiness  \\\n","0       cosine      0.05            27           10         0.908971   \n","0  correlation      0.05            17           10         0.908189   \n","0  correlation      0.01            22           10         0.906756   \n","\n","   stability  global_spearman  \n","0   0.991524         0.440600  \n","0   0.899680         0.443168  \n","0   0.883493         0.431101  "],"text/html":["\n","  <div id=\"df-e1a34b9b-96cf-4608-a9ac-1e95cd833dba\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>metric</th>\n","      <th>min_dist</th>\n","      <th>n_components</th>\n","      <th>n_neighbors</th>\n","      <th>trustworthiness</th>\n","      <th>stability</th>\n","      <th>global_spearman</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>cosine</td>\n","      <td>0.05</td>\n","      <td>27</td>\n","      <td>10</td>\n","      <td>0.908971</td>\n","      <td>0.991524</td>\n","      <td>0.440600</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>correlation</td>\n","      <td>0.05</td>\n","      <td>17</td>\n","      <td>10</td>\n","      <td>0.908189</td>\n","      <td>0.899680</td>\n","      <td>0.443168</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>correlation</td>\n","      <td>0.01</td>\n","      <td>22</td>\n","      <td>10</td>\n","      <td>0.906756</td>\n","      <td>0.883493</td>\n","      <td>0.431101</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1a34b9b-96cf-4608-a9ac-1e95cd833dba')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-e1a34b9b-96cf-4608-a9ac-1e95cd833dba button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-e1a34b9b-96cf-4608-a9ac-1e95cd833dba');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-7eb88008-09ab-4d4b-8c94-a8d23fa2dbd2\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7eb88008-09ab-4d4b-8c94-a8d23fa2dbd2')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-7eb88008-09ab-4d4b-8c94-a8d23fa2dbd2 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"results_best","summary":"{\n  \"name\": \"results_best\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"metric\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"correlation\",\n          \"cosine\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"min_dist\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.023094010767585032,\n        \"min\": 0.01,\n        \"max\": 0.05,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.01,\n          0.05\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_components\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 17,\n        \"max\": 27,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          27,\n          17\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_neighbors\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 10,\n        \"max\": 10,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trustworthiness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0011232336008922946,\n        \"min\": 0.9067559058316914,\n        \"max\": 0.9089705873517803,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.9089705873517803\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stability\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05826391119176074,\n        \"min\": 0.883492834418397,\n        \"max\": 0.9915241102306682,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.9915241102306682\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"global_spearman\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.006356653018773664,\n        \"min\": 0.431100826712069,\n        \"max\": 0.44316769084238067,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.4406004101514007\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["scaler = StandardScaler()\n","scaled_embeds = scaler.fit_transform(embeddings_l2)"],"metadata":{"id":"gEcCAkDeWF-7","executionInfo":{"status":"ok","timestamp":1748411843175,"user_tz":-180,"elapsed":103,"user":{"displayName":"Милена","userId":"18103358405036649457"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["reducer = umap.UMAP(\n","    n_components=27,\n","    n_neighbors=10,\n","    min_dist=0.05,\n","    metric='cosine',\n","    random_state=42\n",")"],"metadata":{"id":"1TNUZkBEVfCb","executionInfo":{"status":"ok","timestamp":1748411843187,"user_tz":-180,"elapsed":12,"user":{"displayName":"Милена","userId":"18103358405036649457"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["umap_embeds = reducer.fit_transform(scaled_embeds)"],"metadata":{"id":"zh5DxDKjqHts","executionInfo":{"status":"ok","timestamp":1748411888518,"user_tz":-180,"elapsed":45328,"user":{"displayName":"Милена","userId":"18103358405036649457"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["#### Кластеризация контекстов"],"metadata":{"id":"sIGahMy_Kc5b"}},{"cell_type":"code","source":["class MetaphorClusteringEnsemble:\n","    \"\"\"\n","    Консенсусная кластеризация для выявления метафорических употреблений глаголов\n","    \"\"\"\n","\n","    def __init__(self, data, algorithm_weights=None):\n","        \"\"\"\n","        Инициализация класса\n","\n","        Parameters:\n","        -----------\n","        data : array-like\n","            Матрица признаков (векторные представления употреблений)\n","        algorithm_weights : dict\n","            Веса для каждого алгоритма в консенсусной матрице\n","        \"\"\"\n","        self.data = np.array(data)\n","        self.n_samples = self.data.shape[0]\n","\n","        # Веса алгоритмов (HDBSCAN как самый надежный)\n","        self.weights = algorithm_weights or {\n","            'hdbscan': 0.5,\n","            'dbscan': 0.3,\n","            'spectral': 0.2\n","        }\n","\n","        self.best_params = {}\n","        self.clusterings = {}\n","        self.consensus_matrix = None\n","        self.final_clusters = None\n","\n","    def optimize_dbscan(self, eps_range=None, min_samples_range=None):\n","        \"\"\"\n","        Подбор оптимальных параметров для DBSCAN\n","        \"\"\"\n","        print(\"Оптимизация параметров DBSCAN...\")\n","\n","        if eps_range is None:\n","            # Автоматическое определение диапазона eps на основе k-distance\n","            k = 4  # минимальное количество точек для формирования кластера\n","            nbrs = NearestNeighbors(n_neighbors=k).fit(self.data)\n","            distances, indices = nbrs.kneighbors(self.data)\n","            distances = np.sort(distances[:, k-1], axis=0)\n","\n","            # Используем метод \"локтя\" для определения оптимального eps\n","            knee_point = self._find_knee_point(distances)\n","            eps_range = np.linspace(distances[knee_point] * 0.5, distances[knee_point] * 2, 20)\n","\n","        if min_samples_range is None:\n","            min_samples_range = range(3, min(20, self.n_samples // 10))\n","\n","        best_score = -1\n","        best_params = {}\n","\n","        param_grid = ParameterGrid({\n","            'eps': eps_range,\n","            'min_samples': min_samples_range\n","        })\n","\n","        for params in param_grid:\n","            try:\n","                dbscan = DBSCAN(**params)\n","                labels = dbscan.fit_predict(self.data)\n","\n","                # Проверяем, что есть хотя бы 2 кластера (исключая шум)\n","                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n","                if n_clusters < 2:\n","                    continue\n","\n","                # Оценка качества (без учета точек шума для silhouette)\n","                mask = labels != -1\n","                if np.sum(mask) < 2:\n","                    continue\n","\n","                score = silhouette_score(self.data[mask], labels[mask])\n","\n","                if score > best_score:\n","                    best_score = score\n","                    best_params = params.copy()\n","\n","            except:\n","                continue\n","\n","        self.best_params['dbscan'] = best_params\n","        print(f\"Лучшие параметры DBSCAN: {best_params}, Score: {best_score:.3f}\")\n","\n","    def optimize_hdbscan(self, min_cluster_size_range=None, min_samples_range=None):\n","        \"\"\"\n","        Подбор оптимальных параметров для HDBSCAN\n","        \"\"\"\n","        print(\"Оптимизация параметров HDBSCAN...\")\n","\n","        if min_cluster_size_range is None:\n","            min_cluster_size_range = np.arange(3, 50, 7)\n","\n","        if min_samples_range is None:\n","            min_samples_range = np.arange(1, 20, 5)\n","\n","        best_score = -1\n","        best_params = {}\n","\n","        param_grid = ParameterGrid({\n","            'min_cluster_size': min_cluster_size_range,\n","            'min_samples': min_samples_range,\n","            'cluster_selection_epsilon': [0.0, 0.1, 0.2, 0.5]\n","        })\n","\n","        for params in param_grid:\n","            try:\n","                clusterer = hdbscan.HDBSCAN(**params)\n","                labels = clusterer.fit_predict(self.data)\n","\n","                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n","                if n_clusters < 2:\n","                    continue\n","\n","                mask = labels != -1\n","                if np.sum(mask) < 2:\n","                    continue\n","\n","                # Комбинированная метрика: silhouette + DBCV (если доступно)\n","                silhouette = silhouette_score(self.data[mask], labels[mask])\n","\n","                # HDBSCAN предоставляет собственную метрику качества\n","                try:\n","                    dbcv_score = clusterer.relative_validity_\n","                    if dbcv_score is not None:\n","                        score = 0.4 * silhouette + 0.6 * dbcv_score\n","                    else:\n","                        score = silhouette\n","                except:\n","                    score = silhouette\n","\n","                if score > best_score:\n","                    best_score = score\n","                    best_params = params.copy()\n","\n","            except:\n","                continue\n","\n","        self.best_params['hdbscan'] = best_params\n","        print(f\"Лучшие параметры HDBSCAN: {best_params}, Score: {best_score:.3f}\")\n","\n","    def optimize_spectral(self, max_clusters=None):\n","        \"\"\"\n","        Подбор оптимального количества кластеров для Spectral Clustering\n","        \"\"\"\n","        print(\"Оптимизация параметров Spectral Clustering...\")\n","\n","        if max_clusters is None:\n","            max_clusters = min(20, self.n_samples // 3)\n","\n","        # Анализ собственных значений лапласиана для определения количества кластеров\n","        similarity_matrix = self._compute_similarity_matrix()\n","        L = laplacian(similarity_matrix, normed=True)\n","        eigenvals = np.linalg.eigvals(L.toarray() if hasattr(L, 'toarray') else L)\n","        eigenvals = np.sort(eigenvals)\n","\n","        # Поиск наибольшего разрыва в собственных значениях\n","        gaps = np.diff(eigenvals[:max_clusters])\n","        spectral_suggestion = np.argmax(gaps) + 2\n","\n","        best_score = -1\n","        best_params = {}\n","\n","        # Тестируем диапазон кластеров вокруг предложенного спектральным анализом\n","        k_range = range(max(2, spectral_suggestion - 3),\n","                       min(max_clusters + 1, spectral_suggestion + 5))\n","\n","        for n_clusters in k_range:\n","            try:\n","                for affinity in ['rbf', 'nearest_neighbors']:\n","                    spectral = SpectralClustering(\n","                        n_clusters=n_clusters,\n","                        affinity=affinity,\n","                        random_state=42\n","                    )\n","                    labels = spectral.fit_predict(self.data)\n","\n","                    score = silhouette_score(self.data, labels)\n","\n","                    if score > best_score:\n","                        best_score = score\n","                        best_params = {\n","                            'n_clusters': n_clusters,\n","                            'affinity': affinity\n","                        }\n","\n","            except:\n","                continue\n","\n","        self.best_params['spectral'] = best_params\n","        print(f\"Лучшие параметры Spectral: {best_params}, Score: {best_score:.3f}\")\n","\n","    def _compute_similarity_matrix(self):\n","        \"\"\"Вычисление матрицы сходства для спектральной кластеризации\"\"\"\n","        distances = pdist(self.data, metric='euclidean')\n","        distance_matrix = squareform(distances)\n","\n","        # RBF kernel\n","        gamma = 1.0 / (2 * np.var(distances))\n","        similarity_matrix = np.exp(-gamma * distance_matrix ** 2)\n","\n","        return similarity_matrix\n","\n","    def _find_knee_point(self, values):\n","        \"\"\"Поиск точки 'колена' в отсортированном массиве\"\"\"\n","        n_points = len(values)\n","        all_coords = np.vstack((range(n_points), values)).T\n","\n","        # Первая и последняя точки\n","        first_point = all_coords[0]\n","        last_point = all_coords[-1]\n","\n","        # Вектор от первой к последней точке\n","        line_vec = last_point - first_point\n","        line_vec_norm = line_vec / np.sqrt(np.sum(line_vec**2))\n","\n","        # Векторы от первой точки к каждой точке\n","        vec_from_first = all_coords - first_point\n","\n","        # Проекции на линию\n","        scalar_proj = np.dot(vec_from_first, line_vec_norm.reshape(-1, 1)).flatten()\n","        vec_proj = np.outer(scalar_proj, line_vec_norm)\n","\n","        # Перпендикулярные расстояния\n","        distances = np.sqrt(np.sum((vec_from_first - vec_proj)**2, axis=1))\n","\n","        return np.argmax(distances)\n","\n","    def fit_all_algorithms(self):\n","        \"\"\"Обучение всех алгоритмов с оптимальными параметрами\"\"\"\n","        print(\"\\nОбучение алгоритмов кластеризации...\")\n","\n","        # DBSCAN\n","        if 'dbscan' in self.best_params:\n","            dbscan = DBSCAN(**self.best_params['dbscan'])\n","            self.clusterings['dbscan'] = dbscan.fit_predict(self.data)\n","            print(f\"DBSCAN: {len(set(self.clusterings['dbscan'])) - (1 if -1 in self.clusterings['dbscan'] else 0)} кластеров\")\n","\n","        # HDBSCAN\n","        if 'hdbscan' in self.best_params:\n","            hdbscan_clusterer = hdbscan.HDBSCAN(**self.best_params['hdbscan'])\n","            self.clusterings['hdbscan'] = hdbscan_clusterer.fit_predict(self.data)\n","            print(f\"HDBSCAN: {len(set(self.clusterings['hdbscan'])) - (1 if -1 in self.clusterings['hdbscan'] else 0)} кластеров\")\n","\n","        # Spectral Clustering\n","        if 'spectral' in self.best_params:\n","            spectral = SpectralClustering(**self.best_params['spectral'], random_state=42)\n","            self.clusterings['spectral'] = spectral.fit_predict(self.data)\n","            print(f\"Spectral: {self.best_params['spectral']['n_clusters']} кластеров\")\n","\n","    def compute_consensus_matrix(self):\n","        \"\"\"Вычисление взвешенной консенсусной матрицы\"\"\"\n","        print(\"\\nВычисление консенсусной матрицы...\")\n","\n","        self.consensus_matrix = np.zeros((self.n_samples, self.n_samples))\n","        total_weight = 0\n","\n","        for alg_name, labels in self.clusterings.items():\n","            if alg_name not in self.weights:\n","                continue\n","\n","            weight = self.weights[alg_name]\n","            total_weight += weight\n","\n","            # Создание матрицы совместного вхождения для текущего алгоритма\n","            cooccurrence_matrix = np.zeros((self.n_samples, self.n_samples))\n","\n","            for i in range(self.n_samples):\n","                for j in range(i, self.n_samples):\n","                    # Проверяем, что обе точки не являются шумом (-1)\n","                    if labels[i] != -1 and labels[j] != -1 and labels[i] == labels[j]:\n","                        cooccurrence_matrix[i, j] = 1\n","                        cooccurrence_matrix[j, i] = 1\n","\n","            self.consensus_matrix += weight * cooccurrence_matrix\n","\n","        # Нормализация на общий вес\n","        self.consensus_matrix /= total_weight\n","\n","        print(\"Консенсусная матрица вычислена\")\n","\n","    def extract_final_clusters(self, threshold=0.5):\n","        \"\"\"\n","        Извлечение финальных консенсусных кластеров\n","\n","        Parameters:\n","        -----------\n","        threshold : float\n","            Порог для определения принадлежности к одному кластеру\n","        \"\"\"\n","        print(f\"\\nИзвлечение финальных кластеров (порог: {threshold})...\")\n","\n","        # Создание графа связей на основе консенсусной матрицы\n","        adjacency_matrix = (self.consensus_matrix >= threshold).astype(int)\n","\n","        # Поиск связанных компонент (кластеров)\n","        visited = np.zeros(self.n_samples, dtype=bool)\n","        clusters = []\n","        cluster_id = 0\n","        self.final_clusters = np.full(self.n_samples, -1)  # -1 для неназначенных точек\n","\n","        def dfs(node, current_cluster):\n","            \"\"\"Поиск в глубину для нахождения связанных компонент\"\"\"\n","            visited[node] = True\n","            current_cluster.append(node)\n","\n","            for neighbor in range(self.n_samples):\n","                if adjacency_matrix[node, neighbor] and not visited[neighbor]:\n","                    dfs(neighbor, current_cluster)\n","\n","        for i in range(self.n_samples):\n","            if not visited[i]:\n","                current_cluster = []\n","                dfs(i, current_cluster)\n","\n","                # Сохраняем только кластеры с более чем одной точкой\n","                if len(current_cluster) > 1:\n","                    clusters.append(current_cluster)\n","                    for point in current_cluster:\n","                        self.final_clusters[point] = cluster_id\n","                    cluster_id += 1\n","\n","        n_clusters = len(clusters)\n","        n_noise = np.sum(self.final_clusters == -1)\n","\n","        print(f\"Найдено {n_clusters} консенсусных кластеров\")\n","        print(f\"Точек шума: {n_noise}\")\n","\n","        # Статистика по размерам кластеров\n","        if n_clusters > 0:\n","            cluster_sizes = [len(cluster) for cluster in clusters]\n","            print(f\"Размеры кластеров: мин={min(cluster_sizes)}, макс={max(cluster_sizes)}, средний={np.mean(cluster_sizes):.1f}\")\n","\n","        return self.final_clusters\n","\n","    def fit(self, dbscan_params=None, hdbscan_params=None, spectral_params=None,\n","            consensus_threshold=0.5):\n","        \"\"\"\n","        Полный пайплайн кластеризации\n","\n","        Parameters:\n","        -----------\n","        dbscan_params, hdbscan_params, spectral_params : dict\n","            Параметры для оптимизации каждого алгоритма\n","        consensus_threshold : float\n","            Порог для извлечения финальных кластеров\n","        \"\"\"\n","        print(\"=== ЗАПУСК КОНСЕНСУСНОЙ КЛАСТЕРИЗАЦИИ ===\")\n","\n","        # Оптимизация параметров\n","        self.optimize_dbscan(**(dbscan_params or {}))\n","        self.optimize_hdbscan(**(hdbscan_params or {}))\n","        self.optimize_spectral(**(spectral_params or {}))\n","\n","        # Обучение алгоритмов\n","        self.fit_all_algorithms()\n","\n","        # Построение консенсуса\n","        self.compute_consensus_matrix()\n","\n","        # Извлечение финальных кластеров\n","        final_clusters = self.extract_final_clusters(threshold=consensus_threshold)\n","\n","        return final_clusters"],"metadata":{"id":"3rYWQhpf5VKQ","executionInfo":{"status":"ok","timestamp":1748411888528,"user_tz":-180,"elapsed":4,"user":{"displayName":"Милена","userId":"18103358405036649457"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["def visualize_consensus_clusters(data, clusters, method='both', figsize=(16, 7),\n","                               save_path=None, dpi=300):\n","    \"\"\"\n","    Визуализация результатов консенсусной кластеризации\n","\n","    Parameters:\n","    -----------\n","    data : np.ndarray\n","        Исходные данные (векторные представления)\n","    clusters : np.ndarray\n","        Метки кластеров (-1 для шума)\n","    method : str\n","        Метод снижения размерности: 'pca', 'tsne', или 'both'\n","    figsize : tuple\n","        Размер фигуры\n","    save_path : str\n","        Путь для сохранения графика\n","    dpi : int\n","        Разрешение для сохранения\n","    \"\"\"\n","\n","    # Настройка стиля и цветов\n","    sns.set(style=\"whitegrid\", font_scale=1.0)\n","    palette = sns.color_palette(\"husl\", 8)\n","    ITALIAN_COLOR = palette[0]\n","\n","    # Получаем уникальные кластеры (исключаем шум -1)\n","    unique_clusters = np.unique(clusters)\n","    noise_mask = clusters == -1\n","    n_clusters = len(unique_clusters) - (1 if -1 in unique_clusters else 0)\n","\n","    # Создаем цветовую палитру\n","    cluster_colors = sns.color_palette(\"husl\", n_colors=max(n_clusters, 8))\n","    if n_clusters > 0:\n","        cluster_colors[0] = ITALIAN_COLOR  # Первый кластер всегда итальянский\n","\n","    # Настройка параметров визуализации\n","    POINT_ALPHA = 0.85\n","    NOISE_ALPHA = 0.5\n","    BORDER_ALPHA = 0.9\n","    ANNOTATION_FONT = {'size': 12, 'weight': 'semibold', 'color': 'white'}\n","\n","    # Создаем фигуру\n","    if method == 'both':\n","        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize, facecolor='white')\n","        axes = [ax1, ax2]\n","        methods = ['PCA', 't-SNE']\n","    else:\n","        fig, ax = plt.subplots(figsize=(figsize[0]//2, figsize[1]), facecolor='white')\n","        axes = [ax]\n","        methods = ['PCA' if method == 'pca' else 't-SNE']\n","\n","    # Применяем методы снижения размерности\n","    reducers = []\n","    if 'PCA' in methods:\n","        pca = PCA(n_components=2, random_state=42)\n","        data_pca = pca.fit_transform(data)\n","        reducers.append(('PCA', data_pca, pca))\n","\n","    if 't-SNE' in methods:\n","        tsne = TSNE(n_components=2, random_state=42,\n","                   perplexity=min(30, len(data)//4), init='pca')\n","        data_tsne = tsne.fit_transform(data)\n","        reducers.append(('t-SNE', data_tsne, None))\n","\n","    # Создаем визуализации\n","    for idx, (reduction_name, data_2d, reducer) in enumerate(reducers):\n","        ax = axes[idx]\n","        ax.set_facecolor('white')\n","\n","        # Рисуем шум (если есть)\n","        if np.any(noise_mask):\n","            ax.scatter(data_2d[noise_mask, 0], data_2d[noise_mask, 1],\n","                      c='#d9d9d9', alpha=NOISE_ALPHA, s=25,\n","                      marker='.', edgecolors='none',\n","                      label='Шум')\n","\n","        # Визуализация кластеров\n","        cluster_counter = 1  # Начинаем нумерацию с 1\n","        for cluster_id in unique_clusters:\n","            if cluster_id == -1: continue\n","\n","            mask = clusters == cluster_id\n","            cluster_points = data_2d[mask]\n","            color = cluster_colors[(cluster_counter-1) % len(cluster_colors)]\n","\n","            # Основные точки кластера\n","            ax.scatter(cluster_points[:, 0], cluster_points[:, 1],\n","                     c=[color], alpha=POINT_ALPHA, s=65,\n","                     edgecolors='w', linewidth=0.6,\n","                     label=f'Кластер {cluster_counter}', zorder=3)\n","\n","            # Границы кластера\n","            if len(cluster_points) >= 3:\n","                try:\n","                    hull = ConvexHull(cluster_points)\n","                    hull_points = cluster_points[hull.vertices]\n","\n","                    # Сглаженные границы\n","                    poly = mpatches.Polygon(hull_points, closed=True,\n","                                          fill=False, edgecolor=color,\n","                                          linewidth=1.8, linestyle='-',\n","                                          alpha=BORDER_ALPHA, zorder=2)\n","                    ax.add_patch(poly)\n","                except:\n","                    if len(cluster_points) >= 2:\n","                        cov = np.cov(cluster_points.T)\n","                        vals, vecs = np.linalg.eigh(cov)\n","                        angle = np.degrees(np.arctan2(*vecs[:, 0][::-1]))\n","                        w, h = 3 * np.sqrt(vals)\n","\n","                        ellipse = Ellipse(cluster_points.mean(0), w, h, angle,\n","                                         fill=False, edgecolor=color,\n","                                         linewidth=1.8, alpha=BORDER_ALPHA)\n","                        ax.add_patch(ellipse)\n","\n","            # Аннотация кластера\n","            center = cluster_points.mean(axis=0)\n","            ax.text(center[0], center[1], str(cluster_counter),\n","                   ha='center', va='center', **ANNOTATION_FONT,\n","                   bbox=dict(boxstyle=\"circle,pad=0.3\",\n","                            facecolor=color, alpha=0.9,\n","                            edgecolor='none'))\n","\n","            cluster_counter += 1\n","\n","        # Заголовки и подзаголовки\n","        ax.set_title(f'Консенсусные кластеры\\n(Метод проекции: {reduction_name})',\n","                    fontsize=13, pad=12)\n","\n","        # Оформление осей\n","        ax.set_xlabel(f'{reduction_name} 1', labelpad=10)\n","        ax.set_ylabel(f'{reduction_name} 2', labelpad=10)\n","        ax.tick_params(axis='both', which='both', length=0)\n","\n","        # Добавление легенды\n","        if idx == 0:\n","            handles = [plt.Line2D([0], [0], marker='o', color='w',\n","                      markerfacecolor=c, markersize=10, alpha=0.8)\n","                     for i, c in enumerate(cluster_colors) if i < n_clusters]\n","            labels = [f'Кластер {i+1}' for i in range(n_clusters)]\n","            if np.any(noise_mask):\n","                handles.append(plt.Line2D([0], [0], marker='.', color='w',\n","                              markerfacecolor='#d9d9d9', markersize=10))\n","                labels.append('Шум')\n","\n","            legend = ax.legend(handles, labels, loc='upper right',\n","                             bbox_to_anchor=(1.15, 1.02),\n","                             frameon=True, framealpha=0.95,\n","                             borderpad=1)\n","            legend.get_frame().set_edgecolor('#333333')\n","\n","        # Объясненная дисперсия для PCA\n","        if reduction_name == 'PCA' and reducer is not None:\n","            explained_var = reducer.explained_variance_ratio_\n","            ax.text(0.03, 0.97,\n","                   f'Expl. Variance:\\nPC1: {explained_var[0]:.1%}\\nPC2: {explained_var[1]:.1%}',\n","                   transform=ax.transAxes, ha='left', va='top',\n","                   fontsize=10, bbox=dict(facecolor='white', alpha=0.8,\n","                                       edgecolor='none', pad=6))\n","\n","    # Общий заголовок\n","    if method == 'both':\n","        plt.suptitle('Результаты консенсусной кластеризации',\n","                   fontsize=16, y=1.02, weight='bold')\n","\n","    # Финализация\n","    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n","\n","    if save_path:\n","        plt.savefig(save_path, dpi=dpi, bbox_inches='tight',\n","                   facecolor='white', transparent=False)\n","        print(f\"График сохранён: {save_path}\")\n","\n","    plt.show()\n","\n","    # Вывод статистики\n","    print(\"\\n=== СТАТИСТИКА ===\")\n","    print(f\"Кластеров: {n_clusters}\")\n","    print(f\"Шум: {noise_mask.sum()} точек\")\n","    print(f\"Всего данных: {len(data)}\")\n","\n","    if n_clusters > 0:\n","        sizes = [np.sum(clusters == c) for c in unique_clusters if c != -1]\n","        print(f\"\\nРазмеры кластеров:\")\n","        print(pd.Series(sizes).describe().to_string())"],"metadata":{"executionInfo":{"status":"ok","timestamp":1748411888640,"user_tz":-180,"elapsed":114,"user":{"displayName":"Милена","userId":"18103358405036649457"}},"id":"EzAeMNZjvf5i"},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["def analyze_cluster_medoids(data, clusters, contexts=None, top_k=20,\n","                           distance_metric='euclidean', return_indices=False):\n","    \"\"\"\n","    Анализ медоидов кластеров и поиск ближайших к ним контекстов\n","\n","    Parameters:\n","    -----------\n","    data : np.ndarray\n","        Матрица признаков (векторные представления)\n","    clusters : np.ndarray\n","        Метки кластеров для каждой точки (-1 для шума)\n","    contexts : list or pd.Series, optional\n","        Список контекстов/предложений соответствующих каждой точке данных\n","    top_k : int, default=20\n","        Количество ближайших контекстов для вывода\n","    distance_metric : str, default='euclidean'\n","        Метрика расстояния для поиска медоида\n","    return_indices : bool, default=False\n","        Возвращать ли индексы медоидов и ближайших точек\n","\n","    Returns:\n","    --------\n","    dict : Словарь с результатами анализа для каждого кластера\n","    \"\"\"\n","\n","    # Проверяем входные данные\n","    if len(data) != len(clusters):\n","        raise ValueError(\"Размеры data и clusters должны совпадать\")\n","\n","    if contexts is not None and len(contexts) != len(data):\n","        raise ValueError(\"Размеры contexts и data должны совпадать\")\n","\n","    # Получаем уникальные кластеры (исключаем шум -1)\n","    unique_clusters = np.unique(clusters)\n","    valid_clusters = unique_clusters[unique_clusters != -1]\n","\n","    results = {}\n","\n","    print(\"=== АНАЛИЗ МЕДОИДОВ КЛАСТЕРОВ ===\\n\")\n","\n","    for cluster_id in valid_clusters:\n","        print(f\" КЛАСТЕР {cluster_id}\")\n","        print(\"=\" * 50)\n","\n","        # Получаем точки текущего кластера\n","        cluster_mask = clusters == cluster_id\n","        cluster_data = data[cluster_mask]\n","        cluster_indices = np.where(cluster_mask)[0]\n","        cluster_size = len(cluster_data)\n","\n","        print(f\"Размер кластера: {cluster_size} точек\")\n","\n","        # Находим медоид - точку с минимальной суммой расстояний до всех остальных в кластере\n","        if cluster_size == 1:\n","            medoid_idx_in_cluster = 0\n","            medoid_distance_sum = 0.0\n","        else:\n","            # Вычисляем матрицу расстояний внутри кластера\n","            intra_distances = pairwise_distances(cluster_data, metric=distance_metric)\n","\n","            # Находим точку с минимальной суммой расстояний (медоид)\n","            distance_sums = np.sum(intra_distances, axis=1)\n","            medoid_idx_in_cluster = np.argmin(distance_sums)\n","            medoid_distance_sum = distance_sums[medoid_idx_in_cluster]\n","\n","        # Индекс медоида в исходных данных\n","        medoid_global_idx = cluster_indices[medoid_idx_in_cluster]\n","        medoid_vector = data[medoid_global_idx]\n","\n","        print(f\"Медоид: точка #{medoid_global_idx}\")\n","        print(f\"Средняя внутрикластерная дистанция медоида: {medoid_distance_sum / cluster_size:.4f}\")\n","\n","        # Показываем контекст медоида, если есть\n","        if contexts is not None:\n","            print(f\"\\n КОНТЕКСТ МЕДОИДА:\")\n","            print(f\"   \\\"{contexts[medoid_global_idx]}\\\"\\n\")\n","\n","        # Находим top_k ближайших точек к медоиду из всего датасета\n","        distances_to_medoid = cdist([medoid_vector], data, metric=distance_metric)[0]\n","\n","        # Сортируем по расстоянию и берем top_k\n","        nearest_indices = np.argsort(distances_to_medoid)[:top_k]\n","        nearest_distances = distances_to_medoid[nearest_indices]\n","\n","        print(f\"ТОП-{top_k} БЛИЖАЙШИХ КОНТЕКСТОВ К МЕДОИДУ:\")\n","        print(\"-\" * 60)\n","\n","        # Сохраняем результаты для кластера\n","        cluster_results = {\n","            'medoid_index': medoid_global_idx,\n","            'medoid_vector': medoid_vector,\n","            'cluster_size': cluster_size,\n","            'avg_intra_distance': medoid_distance_sum / cluster_size,\n","            'nearest_indices': nearest_indices,\n","            'nearest_distances': nearest_distances\n","        }\n","\n","        if contexts is not None:\n","            cluster_results['medoid_context'] = contexts[medoid_global_idx]\n","            cluster_results['nearest_contexts'] = [contexts[idx] for idx in nearest_indices]\n","\n","            # Выводим ближайшие контексты\n","            for rank, (idx, dist) in enumerate(zip(nearest_indices, nearest_distances), 1):\n","                # Отмечаем, к какому кластеру принадлежит каждая точка\n","                point_cluster = clusters[idx]\n","                cluster_marker = f\"[К{point_cluster}]\" if point_cluster != -1 else \"[ШУМ]\"\n","\n","                # Особо выделяем медоид\n","                is_medoid = \"МЕДОИД\" if idx == medoid_global_idx else \"\"\n","\n","                print(f\"{rank:2d}. {cluster_marker} (дист: {dist:.4f}) {is_medoid}\")\n","                print(f\"    \\\"{contexts[idx]}\\\"\")\n","                print()\n","        else:\n","            # Если контекстов нет, просто показываем индексы и расстояния\n","            for rank, (idx, dist) in enumerate(zip(nearest_indices, nearest_distances), 1):\n","                point_cluster = clusters[idx]\n","                cluster_marker = f\"[К{point_cluster}]\" if point_cluster != -1 else \"[ШУМ]\"\n","                is_medoid = \"МЕДОИД\" if idx == medoid_global_idx else \"\"\n","\n","                print(f\"{rank:2d}. Точка #{idx} {cluster_marker} (дист: {dist:.4f}) {is_medoid}\")\n","\n","        results[cluster_id] = cluster_results\n","        print(\"\\n\" + \"=\"*80 + \"\\n\")\n","\n","    # Выводим общую статистику\n","    print(\"ОБЩАЯ СТАТИСТИКА ПО КЛАСТЕРАМ\")\n","    print(\"=\" * 50)\n","\n","    for cluster_id, result in results.items():\n","        print(f\"Кластер {cluster_id}: {result['cluster_size']} точек, \"\n","              f\"средняя дистанция до медоида: {result['avg_intra_distance']:.4f}\")\n","\n","    if return_indices:\n","        return results\n","    else:\n","        # Возвращаем упрощенную версию без векторов для удобства\n","        simple_results = {}\n","        for cluster_id, result in results.items():\n","            simple_results[cluster_id] = {\n","                'medoid_index': result['medoid_index'],\n","                'cluster_size': result['cluster_size'],\n","                'avg_intra_distance': result['avg_intra_distance'],\n","                'nearest_indices': result['nearest_indices'].tolist(),\n","                'nearest_distances': result['nearest_distances'].tolist()\n","            }\n","            if contexts is not None:\n","                simple_results[cluster_id]['medoid_context'] = result['medoid_context']\n","                simple_results[cluster_id]['nearest_contexts'] = result['nearest_contexts']\n","\n","        return simple_results\n","\n","def compare_medoids_similarity(results, data, distance_metric='euclidean'):\n","    \"\"\"\n","    Дополнительная функция для анализа схожести между медоидами разных кластеров\n","\n","    Parameters:\n","    -----------\n","    results : dict\n","        Результаты функции analyze_cluster_medoids с return_indices=True\n","    data : np.ndarray\n","        Исходная матрица данных\n","    distance_metric : str\n","        Метрика для вычисления расстояний\n","    \"\"\"\n","\n","    if len(results) < 2:\n","        print(\"Недостаточно кластеров для сравнения медоидов\")\n","        return\n","\n","    print(\"\\n МАТРИЦА РАССТОЯНИЙ МЕЖДУ МЕДОИДАМИ\")\n","    print(\"=\" * 60)\n","\n","    cluster_ids = list(results.keys())\n","    medoid_vectors = np.array([results[cid]['medoid_vector'] for cid in cluster_ids])\n","\n","    # Вычисляем матрицу расстояний между медоидами\n","    medoid_distances = pairwise_distances(medoid_vectors, metric=distance_metric)\n","\n","    # Создаем красивую таблицу\n","    df_distances = pd.DataFrame(medoid_distances,\n","                               index=[f\"К{cid}\" for cid in cluster_ids],\n","                               columns=[f\"К{cid}\" for cid in cluster_ids])\n","\n","    print(df_distances.round(4))\n","\n","    # Находим самые близкие и далекие пары\n","    mask = np.triu(np.ones_like(medoid_distances, dtype=bool), k=1)\n","    upper_distances = medoid_distances[mask]\n","\n","    if len(upper_distances) > 0:\n","        min_dist_idx = np.unravel_index(np.argmin(medoid_distances + np.eye(len(cluster_ids)) * 1000),\n","                                       medoid_distances.shape)\n","        max_dist_idx = np.unravel_index(np.argmax(medoid_distances), medoid_distances.shape)\n","\n","        print(f\"Самые близкие кластеры: К{cluster_ids[min_dist_idx[0]]} ↔ К{cluster_ids[min_dist_idx[1]]} \"\n","              f\"(расстояние: {medoid_distances[min_dist_idx]:.4f})\")\n","        print(f\"Самые далекие кластеры: К{cluster_ids[max_dist_idx[0]]} ↔ К{cluster_ids[max_dist_idx[1]]} \"\n","              f\"(расстояние: {medoid_distances[max_dist_idx]:.4f})\")"],"metadata":{"id":"hnxmXIBoynQ6","executionInfo":{"status":"ok","timestamp":1748411888642,"user_tz":-180,"elapsed":4,"user":{"displayName":"Милена","userId":"18103358405036649457"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["embeddings_list = umap_embeds.tolist()\n","\n","if len(embeddings_list) == len(it_featured_df):\n","    it_featured_df['embeddings'] = embeddings_list\n","else:\n","    print(\"Количество эмбеддингов не совпадает с количеством строк в DataFrame.\")"],"metadata":{"id":"rUlYk7ZVu1j1","executionInfo":{"status":"ok","timestamp":1748416548431,"user_tz":-180,"elapsed":319,"user":{"displayName":"Милена","userId":"18103358405036649457"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["ensemble = MetaphorClusteringEnsemble(\n","    data=np.array(it_featured_df[it_featured_df['found_verbs'] == 'spingere (толкнуть)']['embeddings'].tolist()),\n","    algorithm_weights={\n","          'hdbscan': 0.5,  # Наибольший вес для HDBSCAN как самого надежного\n","          'dbscan': 0.2,\n","          'spectral': 0.3\n","      }\n",")\n","\n","final_clusters = ensemble.fit(consensus_threshold=0.4)"],"metadata":{"id":"8BDul7xO6Oa1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["visualize_consensus_clusters(\n","     data=np.array(it_featured_df[it_featured_df['found_verbs'] == 'spingere (толкнуть)']['embeddings'].tolist()),\n","     clusters=final_clusters,\n","     method='both'\n",")"],"metadata":{"id":"sdSutQRBvf5j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["detailed_results = analyze_cluster_medoids(\n","    data=np.array(it_featured_df[it_featured_df['found_verbs'] == 'spingere (толкнуть)']['embeddings'].tolist()),\n","    clusters=final_clusters,\n","    contexts=it_featured_df[it_featured_df['found_verbs'] == 'spingere (толкнуть)']['context'].tolist(),\n","    return_indices=True\n",")\n","compare_medoids_similarity(detailed_results, np.array(it_featured_df[it_featured_df['found_verbs'] == 'spingere (толкнуть)']['embeddings'].tolist()))"],"metadata":{"id":"O_DZeIAD0Z9Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qRyTGZTPWUEY"},"source":["## English Tweets"]},{"cell_type":"code","source":["english_df = pd.read_csv(\"/content/drive/MyDrive/Диплом бакалавриат'25/final_english_7to23.csv\")"],"metadata":{"id":"G7FW5LerRUjY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["english_df['found_verbs'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":774},"executionInfo":{"status":"ok","timestamp":1748216111809,"user_tz":-180,"elapsed":6,"user":{"displayName":"Милена","userId":"18103358405036649457"}},"outputId":"de2c811f-7aeb-430f-f089-aa6c5d685f88","id":"anwVV8LORUjZ"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["found_verbs\n","hit (ударить)            9999\n","push (толкнуть)          5633\n","beat (ударить)           4456\n","kick (пнуть)             4261\n","strike (ударить)         2067\n","touch (тронуть)          2025\n","crash (разбиться)         823\n","tap (постучать)           808\n","punch (ударить)           510\n","slap (пощечина)           484\n","bump (столкнуться)        371\n","bang (ударить)            310\n","shove (толкнуть)          222\n","bash (разбить)            193\n","smack (шлёпнуть)          123\n","collide (столкнуться)     110\n","graze (коснуться)          77\n","pat (погладить)            70\n","whack (шлёпнуть)           57\n","swat (поймать)             42\n","nick (царапнуть)           42\n","Name: count, dtype: int64"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>count</th>\n","    </tr>\n","    <tr>\n","      <th>found_verbs</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>hit (ударить)</th>\n","      <td>9999</td>\n","    </tr>\n","    <tr>\n","      <th>push (толкнуть)</th>\n","      <td>5633</td>\n","    </tr>\n","    <tr>\n","      <th>beat (ударить)</th>\n","      <td>4456</td>\n","    </tr>\n","    <tr>\n","      <th>kick (пнуть)</th>\n","      <td>4261</td>\n","    </tr>\n","    <tr>\n","      <th>strike (ударить)</th>\n","      <td>2067</td>\n","    </tr>\n","    <tr>\n","      <th>touch (тронуть)</th>\n","      <td>2025</td>\n","    </tr>\n","    <tr>\n","      <th>crash (разбиться)</th>\n","      <td>823</td>\n","    </tr>\n","    <tr>\n","      <th>tap (постучать)</th>\n","      <td>808</td>\n","    </tr>\n","    <tr>\n","      <th>punch (ударить)</th>\n","      <td>510</td>\n","    </tr>\n","    <tr>\n","      <th>slap (пощечина)</th>\n","      <td>484</td>\n","    </tr>\n","    <tr>\n","      <th>bump (столкнуться)</th>\n","      <td>371</td>\n","    </tr>\n","    <tr>\n","      <th>bang (ударить)</th>\n","      <td>310</td>\n","    </tr>\n","    <tr>\n","      <th>shove (толкнуть)</th>\n","      <td>222</td>\n","    </tr>\n","    <tr>\n","      <th>bash (разбить)</th>\n","      <td>193</td>\n","    </tr>\n","    <tr>\n","      <th>smack (шлёпнуть)</th>\n","      <td>123</td>\n","    </tr>\n","    <tr>\n","      <th>collide (столкнуться)</th>\n","      <td>110</td>\n","    </tr>\n","    <tr>\n","      <th>graze (коснуться)</th>\n","      <td>77</td>\n","    </tr>\n","    <tr>\n","      <th>pat (погладить)</th>\n","      <td>70</td>\n","    </tr>\n","    <tr>\n","      <th>whack (шлёпнуть)</th>\n","      <td>57</td>\n","    </tr>\n","    <tr>\n","      <th>swat (поймать)</th>\n","      <td>42</td>\n","    </tr>\n","    <tr>\n","      <th>nick (царапнуть)</th>\n","      <td>42</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> int64</label>"]},"metadata":{},"execution_count":162}]},{"cell_type":"code","source":["def sample_verbs(df: pd.DataFrame, threshold: int = 460) -> pd.DataFrame:\n","    \"\"\"\n","    Извлекает случайные контексты для глаголов, которые встречаются более заданного количества раз.\n","\n","    :param df: DataFrame с данными\n","    :param threshold: Минимальное количество вхождений для выбора глагола\n","    :return: DataFrame с отобранными контекстами\n","    \"\"\"\n","\n","    verb_counts = df['found_verbs'].value_counts()\n","\n","    common_verbs = verb_counts[verb_counts > threshold].index\n","\n","    sampled_contexts_list = []\n","\n","    for verb in common_verbs:\n","        contexts = df[df['found_verbs'] == verb]\n","\n","        sampled_contexts = contexts.sample(n=min(threshold, len(contexts)), random_state=42)\n","        sampled_contexts_list.append(sampled_contexts)\n","\n","    return pd.concat(sampled_contexts_list, ignore_index=True)\n"],"metadata":{"id":"7oiMSwEpRUja"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["english_contexts = sample_verbs(english_df)"],"metadata":{"id":"eApI99wqRUjb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def enumerate_verbs(df):\n","    df = df.sort_values(by='found_verbs').reset_index(drop=True)\n","\n","    df['context_id'] = df.groupby('found_verbs').cumcount() + 1\n","\n","    df['context_id'] = df['found_verbs'] + '_' + df['context_id'].astype(str)\n","\n","    return df\n","\n","english_contexts = enumerate_verbs(english_contexts)"],"metadata":{"id":"JilsITZSUIfU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["english_contexts = english_contexts[['id', 'context_id', 'user', 'date', 'tweet', 'context', 'found_verbs', 'spacy_word_count']]\n","english_contexts['found_verbs'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":429},"executionInfo":{"status":"ok","timestamp":1748216119787,"user_tz":-180,"elapsed":7,"user":{"displayName":"Милена","userId":"18103358405036649457"}},"outputId":"43fa8509-cff3-43ca-f78a-a08a0d83832e","id":"Vgzw9itsRUjb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["found_verbs\n","beat (ударить)       460\n","crash (разбиться)    460\n","hit (ударить)        460\n","kick (пнуть)         460\n","punch (ударить)      460\n","push (толкнуть)      460\n","slap (пощечина)      460\n","strike (ударить)     460\n","tap (постучать)      460\n","touch (тронуть)      460\n","Name: count, dtype: int64"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>count</th>\n","    </tr>\n","    <tr>\n","      <th>found_verbs</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>beat (ударить)</th>\n","      <td>460</td>\n","    </tr>\n","    <tr>\n","      <th>crash (разбиться)</th>\n","      <td>460</td>\n","    </tr>\n","    <tr>\n","      <th>hit (ударить)</th>\n","      <td>460</td>\n","    </tr>\n","    <tr>\n","      <th>kick (пнуть)</th>\n","      <td>460</td>\n","    </tr>\n","    <tr>\n","      <th>punch (ударить)</th>\n","      <td>460</td>\n","    </tr>\n","    <tr>\n","      <th>push (толкнуть)</th>\n","      <td>460</td>\n","    </tr>\n","    <tr>\n","      <th>slap (пощечина)</th>\n","      <td>460</td>\n","    </tr>\n","    <tr>\n","      <th>strike (ударить)</th>\n","      <td>460</td>\n","    </tr>\n","    <tr>\n","      <th>tap (постучать)</th>\n","      <td>460</td>\n","    </tr>\n","    <tr>\n","      <th>touch (тронуть)</th>\n","      <td>460</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> int64</label>"]},"metadata":{},"execution_count":167}]},{"cell_type":"code","source":["english_contexts.to_csv(\"/content/drive/MyDrive/Диплом бакалавриат'25/english_contexts.csv\")"],"metadata":{"id":"uMBUGEfWRUjb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Sentence-RoBERTa"],"metadata":{"id":"kngpEEqVywtv"}},{"cell_type":"code","source":["model_name = 'sentence-transformers/all-roberta-large-v1'\n","model = AutoModel.from_pretrained(model_name)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"],"metadata":{"id":"W9YK-1Ux_zWV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Предобработка текста"],"metadata":{"id":"fdvyPpEky0bu"}},{"cell_type":"code","source":["english_df = pd.read_csv(\"/content/drive/MyDrive/Диплом бакалавриат'25/english_contexts.csv\")\n","english_df = english_df[['id', 'context_id', 'user', 'date', 'tweet', 'context', 'found_verbs', 'spacy_word_count']]"],"metadata":{"id":"yW_V48uXUWka","executionInfo":{"status":"ok","timestamp":1748455787847,"user_tz":-180,"elapsed":2924,"user":{"displayName":"Милена","userId":"18103358405036649457"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def preprocess_tweet(tweet):\n","    # Удаление URL\n","    tweet = re.sub(r'http\\S+', '', tweet)\n","    # Замена упоминаний\n","    tweet = re.sub(r'@\\w+', '@user', tweet)\n","    # Обработка хэштегов\n","    tweet = re.sub(r'#(\\w+)', lambda x: x.group(1), tweet)\n","    return tweet.strip()\n","\n","english_df['cleaned_text'] = english_df['context'].apply(preprocess_tweet)"],"metadata":{"id":"4B6ejUVPyo6E","executionInfo":{"status":"ok","timestamp":1748455787889,"user_tz":-180,"elapsed":45,"user":{"displayName":"Милена","userId":"18103358405036649457"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["#### Создание эмбеддингов"],"metadata":{"id":"lkKEvo-ISVL9"}},{"cell_type":"code","source":["nlp = spacy.load(\"en_core_web_sm\")\n","target_verbs = np.unique([verb.split()[0] for verb in english_df['found_verbs'].tolist()])\n","\n","def extract_verbs(df):\n","    \"\"\"Извлечение целевых глаголов из оригинального текста\"\"\"\n","    verb_records = []\n","\n","    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting verbs\"):\n","        doc = nlp(row['context'])\n","        verbs = []\n","\n","        for token in doc:\n","            if token.pos_ == \"VERB\":\n","                for target_verb in target_verbs:\n","                    # Проверка частичного/полного совпадения\n","                    if target_verb in token.lemma_ or token.lemma_ in target_verb:\n","                        verbs.append({\n","                            'raw_text': token.text,  # Оригинальная словоформа\n","                            'pattern': target_verb\n","                        })\n","                        break\n","\n","        verb_records.append(verbs)\n","\n","    df['verbs_info'] = verb_records\n","    return df"],"metadata":{"id":"P83S12iU0hDO","executionInfo":{"status":"ok","timestamp":1748455790041,"user_tz":-180,"elapsed":2104,"user":{"displayName":"Милена","userId":"18103358405036649457"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["english_df = extract_verbs(english_df)"],"metadata":{"id":"i5trRSYt0mCq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_masked_text(df):\n","    \"\"\"Создает отдельные записи для каждого целевого глагола\"\"\"\n","\n","    new_rows = []\n","\n","    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\"):\n","        text = row['cleaned_text']\n","        verbs = row['verbs_info']\n","\n","        # Для каждого глагола создаем отдельную маску\n","        for i, verb_info in enumerate(verbs):\n","            new_row = row.copy()\n","            target_verb = verb_info['raw_text']\n","\n","            # Точная замена только конкретного глагола\n","            masked_text = re.sub(\n","                r'\\b{}\\b'.format(re.escape(target_verb)),\n","                '<mask>',\n","                text,\n","                count=1  # Заменяем только первое вхождение\n","            )\n","\n","            # Проверка успешности замены\n","            if masked_text == text:\n","                continue  # Пропускаем неудачные замены\n","\n","            # Обновляем информацию о глаголах\n","            new_verbs = [v for j, v in enumerate(verbs) if j != i]\n","            new_row['verbs_info'] = verbs\n","            new_row['cleaned_text'] = masked_text\n","            new_row['masked_verb'] = target_verb  # Добавляем новое поле\n","\n","            new_rows.append(new_row)\n","\n","    # Создаем новый DataFrame\n","    return pd.DataFrame(new_rows).reset_index(drop=True)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1748455897595,"user_tz":-180,"elapsed":49,"user":{"displayName":"Милена","userId":"18103358405036649457"}},"id":"bsNPf6ZFNoet"},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["en_featured_df = create_masked_text(english_df)"],"metadata":{"id":"uWx1j9brNoeu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Эксперимент с выбором слоев и методов усреднения"],"metadata":{"id":"sCkJLqNa0ZaW"}},{"cell_type":"code","source":["class RoBERTaEmbeddingExperiment:\n","    \"\"\"\n","    Класс для экспериментов с различными методами извлечения эмбеддингов из RoBERTa-large\n","    \"\"\"\n","\n","    def __init__(self, model_name=\"roberta-large\"):\n","        \"\"\"Инициализация модели и токенизатора\"\"\"\n","        print(\"Загрузка модели RoBERTa-large...\")\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        self.model = AutoModelForMaskedLM.from_pretrained(model_name, output_hidden_states=True)\n","        self.model.eval()\n","        self.n_layers = self.model.config.num_hidden_layers  # 24 слоя\n","\n","        # Загружаем spaCy для анализа POS-тегов\n","        self.nlp = spacy.load(\"en_core_web_sm\")\n","\n","        # Определяем методы извлечения эмбеддингов\n","        self.extraction_methods = {\n","            # Стандартные методы\n","            'last_4_avg': self._extract_last_4_average,\n","            'last_2_avg': self._extract_last_2_average,\n","            'weighted_top_4': self._extract_weighted_top_4,\n","\n","            # Средние слои\n","            'middle_layers_avg': self._extract_middle_layers,\n","            'attention_weighted_middle': self._extract_attention_weighted_middle,\n","\n","            # Допонительные методы\n","            'exponential_decay': self._extract_exponential_decay,\n","            'semantic_attention': self._extract_semantic_attention,\n","            'layer_dropout_ensemble': self._extract_layer_dropout_ensemble,\n","            'dynamic_layer_selection': self._extract_dynamic_layer_selection,\n","            'contrastive_pooling': self._extract_contrastive_pooling\n","        }\n","\n","        print(\"Модель загружена!\")\n","\n","    def _get_token_embeddings(self, text, return_attention=False):\n","        \"\"\"Базовая функция для получения скрытых состояний всех слоев\"\"\"\n","        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n","\n","        with torch.no_grad():\n","            if return_attention:\n","                outputs = self.model(**inputs, output_attentions=True)\n","                return outputs.hidden_states, outputs.attentions, inputs\n","            else:\n","                outputs = self.model(**inputs)\n","                return outputs.hidden_states, None, inputs\n","\n","    # ========== СТАНДАРТНЫЕ МЕТОДЫ ==========\n","\n","    def _extract_last_4_average(self, text):\n","        \"\"\"Усреднение последних 4 слоев\"\"\"\n","        hidden_states, _, inputs = self._get_token_embeddings(text)\n","\n","        # Берем последние 4 слоя\n","        last_4 = torch.stack(hidden_states[-4:])  # [4, batch, seq_len, hidden_dim]\n","        averaged = torch.mean(last_4, dim=0)  # [batch, seq_len, hidden_dim]\n","\n","        # Усредняем по всем токенам (исключая специальные)\n","        mask = inputs['attention_mask'].bool()\n","        pooled = self._mean_pooling(averaged, mask)\n","\n","        return pooled.squeeze().numpy()\n","\n","    def _extract_last_2_average(self, text):\n","        \"\"\"Усреднение последних 2 слоев\"\"\"\n","        hidden_states, _, inputs = self._get_token_embeddings(text)\n","\n","        last_2 = torch.stack(hidden_states[-2:])\n","        averaged = torch.mean(last_2, dim=0)\n","\n","        mask = inputs['attention_mask'].bool()\n","        pooled = self._mean_pooling(averaged, mask)\n","\n","        return pooled.squeeze().numpy()\n","\n","    def _extract_weighted_top_4(self, text):\n","        \"\"\"Взвешенное усреднение топ-4 слоев с убывающими весами\"\"\"\n","        hidden_states, _, inputs = self._get_token_embeddings(text)\n","\n","        # Веса для последних 4 слоев: [0.4, 0.3, 0.2, 0.1]\n","        weights = torch.tensor([0.1, 0.2, 0.3, 0.4]).unsqueeze(1).unsqueeze(2)\n","\n","        last_4 = torch.stack(hidden_states[-4:])\n","        weighted = torch.sum(last_4 * weights, dim=0)\n","\n","        mask = inputs['attention_mask'].bool()\n","        pooled = self._mean_pooling(weighted, mask)\n","\n","        return pooled.squeeze().numpy()\n","\n","    # ========== СРЕДНИЕ СЛОИ ==========\n","\n","    def _extract_middle_layers(self, text):\n","        \"\"\"Усреднение средних слоев (слои 10-18 из 24)\"\"\"\n","        hidden_states, _, inputs = self._get_token_embeddings(text)\n","\n","        # Берем средние слои (10-17 включительно)\n","        middle_layers = torch.stack(hidden_states[10:18])\n","        averaged = torch.mean(middle_layers, dim=0)\n","\n","        mask = inputs['attention_mask'].bool()\n","        pooled = self._mean_pooling(averaged, mask)\n","        return pooled.squeeze().numpy()\n","\n","    def _extract_attention_weighted_middle(self, text):\n","        \"\"\"Взвешивание средних слоев на основе attention weights\"\"\"\n","        hidden_states, attentions, inputs = self._get_token_embeddings(text, return_attention=True)\n","\n","        # Берем attention веса средних слоев (10-17)\n","        middle_attentions = attentions[10:18]\n","        middle_states = torch.stack(hidden_states[10:18])\n","\n","        # Усредняем attention по головам и используем как веса\n","        attention_weights = torch.stack([att.mean(dim=1) for att in middle_attentions])  # [4, batch, seq_len, seq_len]\n","        attention_weights = attention_weights.mean(dim=-1)  # [4, batch, seq_len]\n","        attention_weights = torch.softmax(attention_weights.mean(dim=-1), dim=0)  # [4, batch]\n","\n","        # Применяем веса к состояниям\n","        weighted_states = torch.sum(middle_states * attention_weights.unsqueeze(-1).unsqueeze(-1), dim=0)\n","\n","        mask = inputs['attention_mask'].bool()\n","        pooled = self._mean_pooling(weighted_states, mask)\n","\n","        return pooled.squeeze().numpy()\n","\n","    # ========== ДОПОЛНИТЕЛЬНЫЕ МЕТОДЫ ==========\n","\n","    def _extract_exponential_decay(self, text):\n","        \"\"\"Экспоненциальное затухание весов от последних к средним слоям\"\"\"\n","        hidden_states, _, inputs = self._get_token_embeddings(text)\n","\n","        # Создаем экспоненциально убывающие веса\n","        num_layers = len(hidden_states)\n","        decay_factor = 0.8\n","        weights = torch.tensor([decay_factor ** (num_layers - i - 1) for i in range(num_layers)])\n","        weights = weights / weights.sum()  # нормализуем\n","\n","        # Исключаем первые 4 слоя (слишком низкоуровневые)\n","        relevant_states = torch.stack(hidden_states[4:])\n","        relevant_weights = weights[4:] / weights[4:].sum()\n","\n","        weighted = torch.sum(relevant_states * relevant_weights.unsqueeze(1).unsqueeze(2), dim=0)\n","\n","        mask = inputs['attention_mask'].bool()\n","        pooled = self._mean_pooling(weighted, mask)\n","\n","        return pooled.squeeze().numpy()\n","\n","    def _extract_semantic_attention(self, text):\n","        \"\"\"Адаптивное взвешивание на основе семантической важности токенов\"\"\"\n","        hidden_states, attentions, inputs = self._get_token_embeddings(text, return_attention=True)\n","\n","        # Берем верхние слои\n","        top_layers = torch.stack(hidden_states[-4:])\n","        top_attentions = attentions[-4:]\n","\n","        # Вычисляем \"семантическую важность\" как дисперсию attention весов\n","        semantic_importance = []\n","        for att in top_attentions:\n","            # att: [batch, num_heads, seq_len, seq_len]\n","            att_variance = torch.var(att.mean(dim=1), dim=-1)  # [batch, seq_len]\n","            semantic_importance.append(att_variance.mean(dim=-1))  # [batch]\n","\n","        semantic_weights = torch.stack(semantic_importance)  # [4, batch]\n","        semantic_weights = torch.softmax(semantic_weights, dim=0)\n","\n","        weighted = torch.sum(top_layers * semantic_weights.unsqueeze(-1).unsqueeze(-1), dim=0)\n","\n","        mask = inputs['attention_mask'].bool()\n","        pooled = self._mean_pooling(weighted, mask)\n","\n","        return pooled.squeeze().numpy()\n","\n","    def _extract_layer_dropout_ensemble(self, text):\n","        \"\"\"Ансамбль с случайным исключением слоев\"\"\"\n","        hidden_states, _, inputs = self._get_token_embeddings(text)\n","\n","        # Создаем несколько ансамблей с разными комбинациями слоев\n","        ensembles = []\n","        relevant_layers = list(range(12, len(hidden_states)))  # средние и верхние слои\n","\n","        for _ in range(5):  # 5 разных ансамблей\n","            # Случайно выбираем 60-80% слоев\n","            num_select = np.random.randint(int(len(relevant_layers) * 0.6), int(len(relevant_layers) * 0.8) + 1)\n","            selected_indices = np.random.choice(relevant_layers, num_select, replace=False)\n","\n","            selected_states = torch.stack([hidden_states[i] for i in selected_indices])\n","            ensemble_avg = torch.mean(selected_states, dim=0)\n","            ensembles.append(ensemble_avg)\n","\n","        # Усредняем все ансамбли\n","        final_ensemble = torch.mean(torch.stack(ensembles), dim=0)\n","\n","        mask = inputs['attention_mask'].bool()\n","        pooled = self._mean_pooling(final_ensemble, mask)\n","\n","        return pooled.squeeze().numpy()\n","\n","    def _extract_dynamic_layer_selection(self, text):\n","        \"\"\"Динамический выбор слоев на основе косинусного сходства\"\"\"\n","        hidden_states, _, inputs = self._get_token_embeddings(text)\n","\n","        # Вычисляем попарное косинусное сходство между слоями\n","        layer_similarities = []\n","        for i in range(6, len(hidden_states) - 1):  # средние и верхние слои\n","            state1 = hidden_states[i].mean(dim=1)  # [batch, hidden_dim]\n","            state2 = hidden_states[i + 1].mean(dim=1)\n","\n","            similarity = torch.cosine_similarity(state1, state2, dim=-1)\n","            layer_similarities.append(similarity.item())\n","\n","        # Выбираем слои с наименьшим сходством (наиболее информативные переходы)\n","        similarity_scores = np.array(layer_similarities)\n","        diverse_indices = np.argsort(similarity_scores)[:4]  # топ-4 наиболее различающихся\n","        diverse_indices = [i + 6 for i in diverse_indices]  # корректируем индексы\n","\n","        selected_states = torch.stack([hidden_states[i] for i in diverse_indices])\n","        averaged = torch.mean(selected_states, dim=0)\n","\n","        mask = inputs['attention_mask'].bool()\n","        pooled = self._mean_pooling(averaged, mask)\n","\n","        return pooled.squeeze().numpy()\n","\n","    def _extract_contrastive_pooling(self, text):\n","        \"\"\"Контрастивное объединение: подчеркиваем различия между слоями\"\"\"\n","        hidden_states, _, inputs = self._get_token_embeddings(text)\n","\n","        # Берем верхние слои\n","        top_layers = hidden_states[-4:]\n","\n","        # Вычисляем \"контрастивные\" веса\n","        contrasts = []\n","        for i, layer in enumerate(top_layers):\n","            if i == 0:\n","                contrasts.append(layer)\n","            else:\n","                # Подчеркиваем различия с предыдущим слоем\n","                contrast = layer - 0.3 * top_layers[i-1]\n","                contrasts.append(contrast)\n","\n","        # Комбинируем контрастивные представления\n","        final_contrast = torch.mean(torch.stack(contrasts), dim=0)\n","\n","        mask = inputs['attention_mask'].bool()\n","        pooled = self._mean_pooling(final_contrast, mask)\n","\n","        return pooled.squeeze().numpy()\n","\n","    def _mean_pooling(self, token_embeddings, attention_mask):\n","        \"\"\"Усреднение токенов с учетом маски внимания\"\"\"\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","        pooled = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","\n","        # Убеждаемся, что возвращаем 2D тензор [batch_size, hidden_dim]\n","        if pooled.dim() == 1:\n","            pooled = pooled.unsqueeze(0)\n","        elif pooled.dim() > 2:\n","            pooled = pooled.view(pooled.size(0), -1)\n","\n","        return pooled\n","\n","    def extract_embeddings_for_method(self, texts, method_name):\n","        \"\"\"Извлечение эмбеддингов для всех текстов с выбранным методом\"\"\"\n","        method = self.extraction_methods[method_name]\n","        embeddings = []\n","\n","        print(f\"Извлечение эмбеддингов методом '{method_name}'...\")\n","        for text in tqdm(texts, desc=f\"Обработка {method_name}\",\n","                        leave=True, ncols=100, file=sys.stdout):\n","            try:\n","                embedding = method(text)\n","\n","                # Проверяем и исправляем размерность\n","                if isinstance(embedding, np.ndarray):\n","                    if embedding.ndim == 1:\n","                        embeddings.append(embedding)\n","                    elif embedding.ndim == 2:\n","                        # Если 2D, берем первую строку (batch_size=1)\n","                        embeddings.append(embedding[0])\n","                    else:\n","                        # Если больше 2D, сглаживаем\n","                        embeddings.append(embedding.flatten()[:768])  # обрезаем до нужной размерности\n","                else:\n","                    # Если не numpy array, конвертируем\n","                    embedding = np.array(embedding)\n","                    if embedding.ndim > 1:\n","                        embedding = embedding.flatten()[:768]\n","                    embeddings.append(embedding)\n","\n","            except Exception as e:\n","                print(f\"Ошибка для текста: {text[:50]}... - {e}\")\n","                # Добавляем нулевой вектор в случае ошибки\n","                embeddings.append(np.zeros(768))\n","\n","        # Финальная проверка размерности\n","        embeddings_array = np.array(embeddings)\n","\n","        # Если все еще есть проблемы с размерностью, приводим к правильной форме\n","        if embeddings_array.ndim != 2:\n","            print(f\"Исправление размерности массива: {embeddings_array.shape} -> (?, 768)\")\n","            embeddings_list = []\n","            for emb in embeddings:\n","                if isinstance(emb, np.ndarray):\n","                    if emb.ndim == 0:\n","                        embeddings_list.append(np.zeros(768))\n","                    elif emb.ndim == 1:\n","                        if len(emb) >= 768:\n","                            embeddings_list.append(emb[:768])\n","                        else:\n","                            padded = np.zeros(768)\n","                            padded[:len(emb)] = emb\n","                            embeddings_list.append(padded)\n","                    else:\n","                        flattened = emb.flatten()\n","                        if len(flattened) >= 768:\n","                            embeddings_list.append(flattened[:768])\n","                        else:\n","                            padded = np.zeros(768)\n","                            padded[:len(flattened)] = flattened\n","                            embeddings_list.append(padded)\n","                else:\n","                    embeddings_list.append(np.zeros(768))\n","            embeddings_array = np.array(embeddings_list)\n","\n","        print(f\"Итоговая размерность эмбеддингов: {embeddings_array.shape}\")\n","        return embeddings_array\n","\n","    def evaluate_clustering_quality(self, embeddings, labels=None):\n","        \"\"\"Оценка качества кластеризации\"\"\"\n","\n","        # Проверяем размерность входных данных\n","        print(f\"Проверка размерности эмбеддингов: {embeddings.shape}\")\n","\n","        if embeddings.ndim != 2:\n","            print(f\"Неправильная размерность эмбеддингов: {embeddings.shape}\")\n","            return {\n","                'silhouette_score': -1,\n","                'davies_bouldin_score': float('inf'),\n","                'calinski_harabasz_score': 0,\n","                'trustworthiness': 0,\n","                'n_clusters': 0,\n","                'noise_ratio': 1.0,\n","                'umap_embeddings': np.zeros((len(embeddings), 2)),\n","                'cluster_labels': np.full(len(embeddings), -1)\n","            }\n","\n","        # Проверяем на NaN и inf\n","        if np.any(np.isnan(embeddings)) or np.any(np.isinf(embeddings)):\n","            print(\"Найдены NaN или inf значения в эмбеддингах\")\n","            embeddings = np.nan_to_num(embeddings, nan=0.0, posinf=1.0, neginf=-1.0)\n","\n","        try:\n","            # Применяем UMAP для снижения размерности\n","            umap_reducer = umap.UMAP(\n","                n_components=2,\n","                n_neighbors=min(20, len(embeddings)-1),  # учитываем размер данных\n","                min_dist=0.1,\n","                metric='cosine',\n","                random_state=42\n","            )\n","\n","            umap_embeddings = umap_reducer.fit_transform(embeddings)\n","\n","            # Кластеризация с HDBSCAN\n","            min_cluster_size = max(3, min(5, len(embeddings)//10))  # адаптивный размер\n","            clusterer = HDBSCAN(min_cluster_size=min_cluster_size, metric='euclidean')\n","            cluster_labels = clusterer.fit_predict(umap_embeddings)\n","\n","        except Exception as e:\n","            print(f\"Ошибка при UMAP/кластеризации: {e}\")\n","            return {\n","                'silhouette_score': -1,\n","                'davies_bouldin_score': float('inf'),\n","                'calinski_harabasz_score': 0,\n","                'trustworthiness': 0,\n","                'n_clusters': 0,\n","                'noise_ratio': 1.0,\n","                'umap_embeddings': np.zeros((len(embeddings), 2)),\n","                'cluster_labels': np.full(len(embeddings), -1)\n","            }\n","\n","        # Исключаем шум (-1 метки) для вычисления метрик\n","        valid_mask = cluster_labels != -1\n","        if valid_mask.sum() < 2:\n","            return {\n","                'silhouette_score': -1,\n","                'davies_bouldin_score': float('inf'),\n","                'calinski_harabasz_score': 0,\n","                'trustworthiness': 0,\n","                'n_clusters': 0,\n","                'noise_ratio': 1.0,\n","                'umap_embeddings': umap_embeddings,\n","                'cluster_labels': cluster_labels\n","            }\n","\n","        valid_embeddings = umap_embeddings[valid_mask]\n","        valid_labels = cluster_labels[valid_mask]\n","\n","        # Вычисляем метрики\n","        try:\n","            silhouette = silhouette_score(valid_embeddings, valid_labels)\n","            davies_bouldin = davies_bouldin_score(valid_embeddings, valid_labels)\n","            calinski_harabasz = calinski_harabasz_score(valid_embeddings, valid_labels)\n","        except Exception as e:\n","            print(f\"Ошибка при вычислении метрик: {e}\")\n","            silhouette = -1\n","            davies_bouldin = float('inf')\n","            calinski_harabasz = 0\n","\n","        # Вычисляем trustworthiness\n","        try:\n","            from sklearn.manifold import trustworthiness as sklearn_trustworthiness\n","            trustworthiness = sklearn_trustworthiness(embeddings, umap_embeddings, n_neighbors=min(10, len(embeddings)-1))\n","        except Exception as e:\n","            print(f\"Ошибка при вычислении trustworthiness: {e}\")\n","            trustworthiness = 0\n","\n","        return {\n","            'silhouette_score': silhouette,\n","            'davies_bouldin_score': davies_bouldin,\n","            'calinski_harabasz_score': calinski_harabasz,\n","            'trustworthiness': trustworthiness,\n","            'n_clusters': len(np.unique(valid_labels)),\n","            'noise_ratio': (cluster_labels == -1).mean(),\n","            'umap_embeddings': umap_embeddings,\n","            'cluster_labels': cluster_labels\n","        }\n","\n","    def _calculate_trustworthiness(self, X_original, X_embedded, k=10):\n","        \"\"\"Вычисление trustworthiness score\"\"\"\n","        from sklearn.neighbors import NearestNeighbors\n","\n","        # Находим ближайших соседей в исходном и проецированном пространстве\n","        nn_original = NearestNeighbors(n_neighbors=k+1, metric='cosine').fit(X_original)\n","        nn_embedded = NearestNeighbors(n_neighbors=k+1, metric='euclidean').fit(X_embedded)\n","\n","        _, indices_original = nn_original.kneighbors(X_original)\n","        _, indices_embedded = nn_embedded.kneighbors(X_embedded)\n","\n","        n = X_original.shape[0]\n","        trustworthiness = 0\n","\n","        for i in range(n):\n","            # Исключаем сам элемент (первый в списке соседей)\n","            neighbors_original = set(indices_original[i][1:])\n","            neighbors_embedded = set(indices_embedded[i][1:])\n","\n","            # Находим \"интрудеров\" - точки, которые близки в проекции, но далеки в оригинале\n","            intruders = neighbors_embedded - neighbors_original\n","\n","            rank_sum = 0\n","            for intruder in intruders:\n","                # Находим ранг интрудера в исходном пространстве\n","                rank = np.where(indices_original[i] == intruder)[0]\n","                if len(rank) > 0:\n","                    rank_sum += max(0, rank[0] - k)\n","\n","            trustworthiness += rank_sum\n","\n","        # Нормализация\n","        max_sum = k * (2 * n - 3 * k - 1) / 2\n","        trustworthiness = 1 - (2 * trustworthiness) / (n * max_sum) if max_sum > 0 else 1\n","\n","        return max(0, trustworthiness)\n","\n","    def run_comprehensive_experiment(self, df, text_column='cleaned_text', verb_column='masked_verb'):\n","        \"\"\"Запуск полного эксперимента со всеми методами\"\"\"\n","        texts = df[text_column].tolist()\n","        verbs = df[verb_column].tolist() if verb_column in df.columns else None\n","\n","        print(f\"Начинаем эксперимент с {len(texts)} текстами и {len(self.extraction_methods)} методами\")\n","        print(f\"Методы: {list(self.extraction_methods.keys())}\")\n","\n","        results = {}\n","        embeddings_cache = {}\n","\n","        # Извлекаем эмбеддинги для каждого метода\n","        for method_name in tqdm(self.extraction_methods.keys(),\n","                               desc=\"Обработка методов\", leave=True, ncols=100):\n","            embeddings = self.extract_embeddings_for_method(texts, method_name)\n","            embeddings_cache[method_name] = embeddings\n","\n","            # Оцениваем качество кластеризации\n","            print(f\"Оценка качества для метода '{method_name}'...\")\n","            quality_metrics = self.evaluate_clustering_quality(embeddings, verbs)\n","\n","            results[method_name] = {\n","                'embeddings': embeddings,\n","                'metrics': quality_metrics\n","            }\n","\n","            print(f\"{method_name}: Silhouette={quality_metrics['silhouette_score']:.3f}, \"\n","                  f\"Clusters={quality_metrics['n_clusters']}, \"\n","                  f\"Trustworthiness={quality_metrics['trustworthiness']:.3f}\")\n","\n","        return results\n","\n","    def plot_comparison_results(self, results, save_path=None):\n","          \"\"\"\n","          Визуализация результатов сравнения методов извлечения эмбеддингов.\n","\n","          Args:\n","              results (dict): Результаты выполнения экспериментов\n","              save_path (str, optional): Путь для сохранения графиков\n","\n","          Returns:\n","              list: Ранжированный список методов по композитному рейтингу\n","          \"\"\"\n","          # Настройка стиля графиков\n","          sns.set(style=\"whitegrid\")\n","          palette = sns.color_palette(\"husl\", 8)\n","          plt.figure(figsize=(14, 10))\n","\n","\n","    def plot_comparison_results(self, results, save_path=None):\n","          \"\"\"Визуализация результатов сравнения методов\"\"\"\n","          # Подготавливаем данные для сравнения\n","          methods = list(results.keys())\n","          metrics_data = {\n","              'Method': [],\n","              'Silhouette Score': [],\n","              'Davies-Bouldin Score': [],\n","              'Calinski-Harabasz Score': [],\n","              'Trustworthiness': [],\n","              'N Clusters': [],\n","              'Noise Ratio': []\n","          }\n","\n","          for method in methods:\n","              metrics = results[method]['metrics']\n","              metrics_data['Method'].append(method)\n","              metrics_data['Silhouette Score'].append(metrics['silhouette_score'])\n","              metrics_data['Davies-Bouldin Score'].append(metrics['davies_bouldin_score'])\n","              metrics_data['Calinski-Harabasz Score'].append(metrics['calinski_harabasz_score'])\n","              metrics_data['Trustworthiness'].append(metrics['trustworthiness'])\n","              metrics_data['N Clusters'].append(metrics['n_clusters'])\n","              metrics_data['Noise Ratio'].append(metrics['noise_ratio'])\n","\n","          # Создаем большую фигуру для всех графиков\n","          fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n","          fig.suptitle('Сравнение методов извлечения эмбеддингов UmBERTo', fontsize=16, fontweight='bold')\n","\n","          # График 1: Silhouette Score\n","          ax1 = axes[0, 0]\n","          bars1 = ax1.bar(range(len(methods)), metrics_data['Silhouette Score'],\n","                        color=palette[:len(methods)])\n","          ax1.set_title('Silhouette Score (выше = лучше)')\n","          ax1.set_xlabel('Методы')\n","          ax1.set_ylabel('Score')\n","          ax1.set_xticks(range(len(methods)))\n","          ax1.set_xticklabels(methods, rotation=45, ha='right')\n","          ax1.grid(True, alpha=0.3)\n","\n","          # График 2: Davies-Bouldin Score\n","          ax2 = axes[0, 1]\n","          bars2 = ax2.bar(range(len(methods)), metrics_data['Davies-Bouldin Score'],\n","                        color=palette[:len(methods)])\n","          ax2.set_title('Davies-Bouldin Score (ниже = лучше)')\n","          ax2.set_xlabel('Методы')\n","          ax2.set_ylabel('Score')\n","          ax2.set_xticks(range(len(methods)))\n","          ax2.set_xticklabels(methods, rotation=45, ha='right')\n","          ax2.grid(True, alpha=0.3)\n","\n","          # График 3: Trustworthiness\n","          ax3 = axes[0, 2]\n","          bars3 = ax3.bar(range(len(methods)), metrics_data['Trustworthiness'],\n","                        color=palette[:len(methods)])\n","          ax3.set_title('Trustworthiness (выше = лучше)')\n","          ax3.set_xlabel('Методы')\n","          ax3.set_ylabel('Score')\n","          ax3.set_xticks(range(len(methods)))\n","          ax3.set_xticklabels(methods, rotation=45, ha='right')\n","          ax3.grid(True, alpha=0.3)\n","\n","          # График 4: Количество кластеров\n","          ax4 = axes[1, 0]\n","          bars4 = ax4.bar(range(len(methods)), metrics_data['N Clusters'],\n","                        color=palette[:len(methods)])\n","          ax4.set_title('Количество кластеров')\n","          ax4.set_xlabel('Методы')\n","          ax4.set_ylabel('Количество')\n","          ax4.set_xticks(range(len(methods)))\n","          ax4.set_xticklabels(methods, rotation=45, ha='right')\n","          ax4.grid(True, alpha=0.3)\n","\n","          # График 5: Доля шума\n","          ax5 = axes[1, 1]\n","          bars5 = ax5.bar(range(len(methods)), metrics_data['Noise Ratio'],\n","                        color=palette[:len(methods)])\n","          ax5.set_title('Доля шума (ниже = лучше)')\n","          ax5.set_xlabel('Методы')\n","          ax5.set_ylabel('Доля')\n","          ax5.set_xticks(range(len(methods)))\n","          ax5.set_xticklabels(methods, rotation=45, ha='right')\n","          ax5.grid(True, alpha=0.3)\n","\n","          # График 6: Комбинированный рейтинг\n","          ax6 = axes[1, 2]\n","          # Нормализуем метрики и создаем составной рейтинг\n","          normalized_silhouette = np.array(metrics_data['Silhouette Score'])\n","          normalized_trustworthiness = np.array(metrics_data['Trustworthiness'])\n","          normalized_db = 1 / (1 + np.array(metrics_data['Davies-Bouldin Score']))  # инвертируем\n","          normalized_noise = 1 - np.array(metrics_data['Noise Ratio'])  # инвертируем\n","\n","          composite_score = (normalized_silhouette + normalized_trustworthiness +\n","                            normalized_db + normalized_noise) / 4\n","\n","          bars6 = ax6.bar(range(len(methods)), composite_score,\n","                        color=palette[:len(methods)])\n","          ax6.set_title('Композитный рейтинг (выше = лучше)')\n","          ax6.set_xlabel('Методы')\n","          ax6.set_ylabel('Score')\n","          ax6.set_xticks(range(len(methods)))\n","          ax6.set_xticklabels(methods, rotation=45, ha='right')\n","          ax6.grid(True, alpha=0.3)\n","\n","          plt.tight_layout()\n","\n","          if save_path:\n","              plt.savefig(save_path, dpi=300, bbox_inches='tight')\n","\n","          plt.show()\n","\n","          # Возвращаем рейтинг методов\n","          ranking = sorted(zip(methods, composite_score), key=lambda x: x[1], reverse=True)\n","          return ranking"],"metadata":{"id":"4Eb6pSf4Mbm4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["experiment = RoBERTaEmbeddingExperiment()\n","\n","# Запуск полного эксперимента\n","results = experiment.run_comprehensive_experiment(\n","    en_featured_df[:200],\n","    text_column='cleaned_text',    # колонка с маскированными текстами\n","    verb_column='masked_verb'      # колонка с замаскированными глаголами\n",")\n","\n","# Визуализация результатов\n","ranking = experiment.plot_comparison_results(results)\n","\n","# Вывод лучших методов\n","print(\"ТОП-3 метода:\")\n","for i, (method, score) in enumerate(ranking[:3], 1):\n","    print(f\"{i}. {method}: {score:.4f}\")"],"metadata":{"id":"Dbc2OIImTxcZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Создание эмбеддингов с помощью лучшего метода агрегации"],"metadata":{"id":"DUQ4h1DDN9xZ"}},{"cell_type":"code","source":["def generate_dynamic_layer_embeddings(\n","    texts,\n","    model,\n","    tokenizer,\n","    max_length=512,\n","    device=\"cpu\",\n","    batch_size=16,\n","    show_progress=True,\n","    n_layers=4\n","):\n","    \"\"\"\n","    Генерирует эмбеддинги с динамическим выбором слоев\n","\n","    Параметры:\n","        texts (list): Список текстов для обработки\n","        model: Предварительно загруженная модель\n","        tokenizer: Предварительно загруженный токенизатор\n","        max_length (int): Максимальная длина текста\n","        device (str): Устройство для вычислений (cpu/cuda)\n","        batch_size (int): Размер пакета обработки\n","        show_progress (bool): Показывать прогресс-бар\n","        n_layers (int): Количество слоев для выбора\n","    \"\"\"\n","    # Валидация входных данных\n","    if not texts:\n","        return np.array([])\n","\n","    # Подготовка прогресс-бара\n","    total_batches = (len(texts) + batch_size - 1) // batch_size\n","    pbar = tqdm(total=total_batches, desc=\"Generating embeddings\", disable=not show_progress)\n","\n","    all_embeddings = []\n","\n","    # Обработка батчами\n","    for i in range(0, len(texts), batch_size):\n","        batch_texts = texts[i:i+batch_size]\n","\n","        # Токенизация батча\n","        inputs = tokenizer(\n","            batch_texts,\n","            return_tensors=\"pt\",\n","            padding=True,\n","            truncation=True,\n","            max_length=max_length\n","        ).to(device)\n","\n","        # Получение скрытых состояний\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","            hidden_states = outputs.hidden_states\n","\n","        # Вычисление косинусной схожести между слоями\n","        layer_similarities = []\n","        for i in range(6, len(hidden_states)-1):\n","            state1 = hidden_states[i].mean(dim=1)\n","            state2 = hidden_states[i+1].mean(dim=1)\n","            similarity = torch.cosine_similarity(state1, state2, dim=-1)\n","            layer_similarities.append(similarity.mean().item())\n","\n","        # Выбор наиболее различных слоев\n","        similarity_scores = np.array(layer_similarities)\n","        diverse_indices = np.argsort(similarity_scores)[:n_layers] + 6\n","\n","        # Усреднение выбранных слоев\n","        selected_states = torch.stack([hidden_states[idx] for idx in diverse_indices])\n","        averaged = torch.mean(selected_states, dim=0)\n","\n","        # Пулинг с маской внимания\n","        mask = inputs['attention_mask'].unsqueeze(-1).float()\n","        summed = torch.sum(averaged * mask, dim=1)\n","        counts = torch.clamp(mask.sum(dim=1), min=1e-9)\n","        batch_embeddings = (summed / counts).cpu().numpy()\n","\n","        all_embeddings.append(batch_embeddings)\n","        pbar.update(1)\n","\n","    pbar.close()\n","    return np.vstack(all_embeddings)"],"metadata":{"id":"BxQYni-gN-Y2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForMaskedLM\n","\n","# Загрузка модели и токенизатора\n","model_name = \"sentence-transformers/all-roberta-large-v1\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForMaskedLM.from_pretrained(model_name, output_hidden_states=True)\n","model.eval()"],"metadata":{"collapsed":true,"id":"CmhAP0SqPJ2v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Генерация эмбеддингов\n","embeddings = generate_dynamic_layer_embeddings(\n","    texts=en_featured_df['cleaned_text'].tolist(),\n","    model=model,\n","    tokenizer=tokenizer,\n","    max_length=512,\n","    device=\"cpu\",\n","    batch_size=16\n",")\n","\n","print(f\"Размерность итоговых эмбеддингов: {embeddings.shape}\")\n","print(f\"Пример эмбеддинга: {embeddings[0][:10]}\")"],"metadata":{"id":"eYDQWFAVPGZx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","with open(\"/content/drive/MyDrive/Диплом бакалавриат'25/english_embeddings.pkl\", 'wb') as f:\n","    pickle.dump(embeddings, f)"],"metadata":{"id":"2VZivkSIPrZX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(\"/content/drive/MyDrive/Диплом бакалавриат'25/english_embeddings.pkl\", 'rb') as f:\n","    embeddings = pickle.load(f)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1748455901672,"user_tz":-180,"elapsed":1069,"user":{"displayName":"Милена","userId":"18103358405036649457"}},"id":"w7FJvwAUPrZX"},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["embeddings_np = np.array(embeddings)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1748455901688,"user_tz":-180,"elapsed":13,"user":{"displayName":"Милена","userId":"18103358405036649457"}},"id":"7qCst_5pPrZX"},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# Проверка нормализации\n","norms = np.linalg.norm(embeddings_np, axis=1)\n","print(\"Минимальная норма:\", np.min(norms))\n","print(\"Максимальная норма:\", np.max(norms))"],"metadata":{"id":"a8Kq-41UPrZX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Проверка близости к 1\n","tolerance = 1e-5\n","is_normalized = np.allclose(norms, 1.0, atol=tolerance)\n","print(f\"Все эмбеддинги нормализованы (с погрешностью {tolerance})? {is_normalized}\")"],"metadata":{"id":"e84XA59rPrZX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embeddings_l2 = normalize(embeddings, norm='l2')"],"metadata":{"executionInfo":{"status":"ok","timestamp":1748455901847,"user_tz":-180,"elapsed":113,"user":{"displayName":"Милена","userId":"18103358405036649457"}},"id":"PXp6bxG3PrZY"},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["#### Снижение размерности"],"metadata":{"id":"kjxvoQeKP0O_"}},{"cell_type":"code","source":["param_grid = {\n","    'n_neighbors': np.arange(10, 50, 10),\n","    'min_dist': [0.01, 0.05],\n","    'n_components': np.arange(2, 30, 5),\n","    'metric': ['cosine', 'correlation']\n","}"],"metadata":{"id":"1x8mZWbxP0PA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def bootstrap_stability(embeddings, params, n_samples=5, sample_size=0.8):\n","    stability = []\n","    for _ in range(n_samples):\n","        idx = np.random.choice(len(embeddings), int(len(embeddings)*sample_size))\n","        proj1 = umap.UMAP(**params).fit_transform(embeddings[idx])\n","        proj2 = umap.UMAP(**params).fit_transform(embeddings[idx])\n","        stability.append(spearmanr(proj1.flatten(), proj2.flatten())[0])\n","    return np.mean(stability)"],"metadata":{"id":"9BOGc5PGP0PA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def optimize_umap_params(embeddings, param_grid, sample_size=1000, random_state=42):\n","    \"\"\"\n","    Оптимизирует параметры UMAP для заданных эмбеддингов с использованием грид-поиска.\n","\n","    Параметры:\n","    embeddings (np.array/torch.Tensor): Массив эмбеддингов\n","    param_grid (dict): Сетка параметров для поиска\n","    sample_size (int): Размер выборки для ускорения вычислений\n","    random_state (int): Seed для воспроизводимости\n","\n","    Возвращает:\n","    pd.DataFrame: Результаты экспериментов с метриками\n","    \"\"\"\n","    # Сэмплирование данных\n","    embeddings = embeddings[:sample_size]\n","\n","    # Стандартизация\n","    scaler = StandardScaler()\n","    embeddings_scaled = scaler.fit_transform(embeddings)\n","\n","    # Основной цикл оптимизации\n","    results = []\n","    for params in tqdm(ParameterGrid(param_grid), desc=\"Grid Search\"):\n","        reducer = umap.UMAP(**params, random_state=random_state)\n","        projection = reducer.fit_transform(embeddings_scaled)\n","\n","        # Вычисление метрик\n","        d_orig = pdist(embeddings_scaled, metric=params['metric'])\n","        d_proj = pdist(projection, metric='euclidean')\n","\n","        metrics = {\n","            'trustworthiness': trustworthiness(embeddings_scaled, projection,\n","                                             n_neighbors=params['n_neighbors']),\n","            'stability': bootstrap_stability(embeddings_scaled, params),\n","            'global_spearman': spearmanr(d_orig, d_proj).correlation\n","        }\n","\n","        results.append({**params, **metrics})\n","\n","    return pd.DataFrame(results)\n"],"metadata":{"id":"G34jGtiFP0PA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_df = optimize_umap_params(\n","    embeddings=embeddings_l2,\n","    param_grid=param_grid,\n","    sample_size=460\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748378896498,"user_tz":-180,"elapsed":1388544,"user":{"displayName":"Милена","userId":"18103358405036649457"}},"outputId":"a7d61233-1c52-4c9a-9812-3a18dcf464d1","id":"tjUWwpd9P0PA"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Grid Search: 100%|██████████| 96/96 [23:08<00:00, 14.46s/it]\n"]}]},{"cell_type":"code","source":["def find_best_parameters(results_df, metric_weights=None):\n","    \"\"\"\n","    Находит оптимальные параметры на основе взвешенной комбинации метрик.\n","\n","    Параметры:\n","    results_df (pd.DataFrame): Результаты экспериментов\n","    metric_weights (dict): Веса для метрик (по умолчанию равные веса)\n","\n","    Возвращает:\n","    pd.DataFrame: Топ-5 лучших комбинаций с нормализованными оценками\n","    \"\"\"\n","    df = results_df.copy()\n","\n","    # Задаем веса по умолчанию (равные веса)\n","    if not metric_weights:\n","        metric_weights = {\n","            'trustworthiness': 0.5,\n","            'stability': 0.3,\n","            'global_spearman': 0.2\n","        }\n","\n","    # Нормализуем метрики (Min-Max scaling)\n","    for metric in metric_weights:\n","        df[metric + '_norm'] = (df[metric] - df[metric].min()) / (df[metric].max() - df[metric].min())\n","\n","    # Рассчитываем общий score\n","    df['total_score'] = sum(\n","        df[metric + '_norm'] * weight\n","        for metric, weight in metric_weights.items()\n","    )\n","\n","    # Сортируем по убыванию общего score\n","    best_results = df.sort_values('total_score', ascending=False).head(5)\n","\n","    return best_results\n","\n","best_df = find_best_parameters(results_df)\n","\n","best_df[['n_neighbors', 'min_dist', 'n_components', 'metric',\n","               'trustworthiness', 'stability', 'global_spearman', 'total_score']]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1748378896546,"user_tz":-180,"elapsed":46,"user":{"displayName":"Милена","userId":"18103358405036649457"}},"outputId":"a2d59ac4-466c-49cd-b0d4-7ca6b63d61b0","id":"CmjOACxtP0PA"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    n_neighbors  min_dist  n_components       metric  trustworthiness  \\\n","92           10      0.05            27  correlation         0.865830   \n","44           10      0.05            27       cosine         0.863157   \n","64           10      0.01            22  correlation         0.864753   \n","88           10      0.05            22  correlation         0.865923   \n","76           10      0.05             7  correlation         0.860728   \n","\n","    stability  global_spearman  total_score  \n","92   0.848219         0.445784     0.897042  \n","44   0.859916         0.442754     0.883249  \n","64   0.830985         0.443422     0.880288  \n","88   0.780009         0.449904     0.874859  \n","76   0.853013         0.446975     0.874245  "],"text/html":["\n","  <div id=\"df-f2803ffe-0b51-4a3e-ae8e-5aa925f53fd2\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>n_neighbors</th>\n","      <th>min_dist</th>\n","      <th>n_components</th>\n","      <th>metric</th>\n","      <th>trustworthiness</th>\n","      <th>stability</th>\n","      <th>global_spearman</th>\n","      <th>total_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>92</th>\n","      <td>10</td>\n","      <td>0.05</td>\n","      <td>27</td>\n","      <td>correlation</td>\n","      <td>0.865830</td>\n","      <td>0.848219</td>\n","      <td>0.445784</td>\n","      <td>0.897042</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>10</td>\n","      <td>0.05</td>\n","      <td>27</td>\n","      <td>cosine</td>\n","      <td>0.863157</td>\n","      <td>0.859916</td>\n","      <td>0.442754</td>\n","      <td>0.883249</td>\n","    </tr>\n","    <tr>\n","      <th>64</th>\n","      <td>10</td>\n","      <td>0.01</td>\n","      <td>22</td>\n","      <td>correlation</td>\n","      <td>0.864753</td>\n","      <td>0.830985</td>\n","      <td>0.443422</td>\n","      <td>0.880288</td>\n","    </tr>\n","    <tr>\n","      <th>88</th>\n","      <td>10</td>\n","      <td>0.05</td>\n","      <td>22</td>\n","      <td>correlation</td>\n","      <td>0.865923</td>\n","      <td>0.780009</td>\n","      <td>0.449904</td>\n","      <td>0.874859</td>\n","    </tr>\n","    <tr>\n","      <th>76</th>\n","      <td>10</td>\n","      <td>0.05</td>\n","      <td>7</td>\n","      <td>correlation</td>\n","      <td>0.860728</td>\n","      <td>0.853013</td>\n","      <td>0.446975</td>\n","      <td>0.874245</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f2803ffe-0b51-4a3e-ae8e-5aa925f53fd2')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-f2803ffe-0b51-4a3e-ae8e-5aa925f53fd2 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-f2803ffe-0b51-4a3e-ae8e-5aa925f53fd2');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-c83402e1-0122-4638-8b02-a6cc75212a49\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c83402e1-0122-4638-8b02-a6cc75212a49')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-c83402e1-0122-4638-8b02-a6cc75212a49 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"               'trustworthiness', 'stability', 'global_spearman', 'total_score']]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"n_neighbors\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 10,\n        \"max\": 10,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"min_dist\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.01788854381999832,\n        \"min\": 0.01,\n        \"max\": 0.05,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.01\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_components\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 7,\n        \"max\": 27,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          27\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"metric\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"cosine\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trustworthiness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0021795377821563445,\n        \"min\": 0.8607277351200665,\n        \"max\": 0.865923118305864,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.8631569423387294\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stability\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03224327970770895,\n        \"min\": 0.7800092934629524,\n        \"max\": 0.8599164779610124,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.8599164779610124\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"global_spearman\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0028790902329201206,\n        \"min\": 0.44275386105680703,\n        \"max\": 0.4499043164490089,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.44275386105680703\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00924455815935074,\n        \"min\": 0.8742450608181842,\n        \"max\": 0.8970415742955881,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.8832488069012793\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["param_grid = {\n","    'n_neighbors': [10],\n","    'min_dist': [0.05],\n","    'n_components': [27],\n","    'metric': ['correlation']\n","}"],"metadata":{"id":"HwjuRTyjP0PA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_df_1 = optimize_umap_params(\n","    embeddings=embeddings_l2,\n","    param_grid=param_grid,\n","    sample_size=len(embeddings_np)\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748379819550,"user_tz":-180,"elapsed":591386,"user":{"displayName":"Милена","userId":"18103358405036649457"}},"outputId":"e617e60f-b577-4c8d-a279-08f6d800d2ef","id":"JkSF25lHP0PB"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Grid Search: 100%|██████████| 1/1 [09:51<00:00, 591.07s/it]\n"]}]},{"cell_type":"code","source":["param_grid = {\n","    'n_neighbors': [10],\n","    'min_dist': [0.05],\n","    'n_components': [27],\n","    'metric': ['cosine']\n","}"],"metadata":{"id":"8l48TvuUP0PB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_df_2 = optimize_umap_params(\n","    embeddings=embeddings_l2,\n","    param_grid=param_grid,\n","    sample_size=len(embeddings_np)\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748380233574,"user_tz":-180,"elapsed":414009,"user":{"displayName":"Милена","userId":"18103358405036649457"}},"outputId":"3f1f938a-2f38-4cb7-c9c0-000b9d72b70c","id":"-JfGDOVsP0PB"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Grid Search: 100%|██████████| 1/1 [06:53<00:00, 413.93s/it]\n"]}]},{"cell_type":"code","source":["param_grid = {\n","    'n_neighbors': [10],\n","    'min_dist': [0.01],\n","    'n_components': [22],\n","    'metric': ['correlation']\n","}"],"metadata":{"id":"Nj5t_gxQP0PB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_df_3 = optimize_umap_params(\n","    embeddings=embeddings_l2,\n","    param_grid=param_grid,\n","    sample_size=len(embeddings_np)\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748380773301,"user_tz":-180,"elapsed":539730,"user":{"displayName":"Милена","userId":"18103358405036649457"}},"outputId":"f092df9b-ce8e-4a76-8f64-649bcffd0394","id":"gFKXwg0mP0PB"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Grid Search: 100%|██████████| 1/1 [08:59<00:00, 539.64s/it]\n"]}]},{"cell_type":"code","source":["results_best = pd.concat([results_df_1, results_df_2, results_df_3], axis=0)\n","results_best"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"status":"ok","timestamp":1748380773320,"user_tz":-180,"elapsed":17,"user":{"displayName":"Милена","userId":"18103358405036649457"}},"outputId":"f9dd927e-f820-45e6-8481-e5183278d4a1","id":"5adPw4CyP0PB"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        metric  min_dist  n_components  n_neighbors  trustworthiness  \\\n","0  correlation      0.05            27           10         0.902189   \n","0       cosine      0.05            27           10         0.902855   \n","0  correlation      0.01            22           10         0.902557   \n","\n","   stability  global_spearman  \n","0   0.996618         0.354140  \n","0   0.927906         0.353618  \n","0   0.754340         0.343341  "],"text/html":["\n","  <div id=\"df-5c616288-65a8-4565-ac71-32e9e8afa472\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>metric</th>\n","      <th>min_dist</th>\n","      <th>n_components</th>\n","      <th>n_neighbors</th>\n","      <th>trustworthiness</th>\n","      <th>stability</th>\n","      <th>global_spearman</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>correlation</td>\n","      <td>0.05</td>\n","      <td>27</td>\n","      <td>10</td>\n","      <td>0.902189</td>\n","      <td>0.996618</td>\n","      <td>0.354140</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>cosine</td>\n","      <td>0.05</td>\n","      <td>27</td>\n","      <td>10</td>\n","      <td>0.902855</td>\n","      <td>0.927906</td>\n","      <td>0.353618</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>correlation</td>\n","      <td>0.01</td>\n","      <td>22</td>\n","      <td>10</td>\n","      <td>0.902557</td>\n","      <td>0.754340</td>\n","      <td>0.343341</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5c616288-65a8-4565-ac71-32e9e8afa472')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-5c616288-65a8-4565-ac71-32e9e8afa472 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-5c616288-65a8-4565-ac71-32e9e8afa472');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-33525230-3977-4a97-8b63-d42bd20715eb\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-33525230-3977-4a97-8b63-d42bd20715eb')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-33525230-3977-4a97-8b63-d42bd20715eb button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"results_best","summary":"{\n  \"name\": \"results_best\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"metric\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"cosine\",\n          \"correlation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"min_dist\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.023094010767585032,\n        \"min\": 0.01,\n        \"max\": 0.05,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.01,\n          0.05\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_components\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 22,\n        \"max\": 27,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          22,\n          27\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_neighbors\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 10,\n        \"max\": 10,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trustworthiness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00033366417824558123,\n        \"min\": 0.9021888798751836,\n        \"max\": 0.9028550035909112,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.9021888798751836\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stability\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12486315669978103,\n        \"min\": 0.754340166697047,\n        \"max\": 0.9966178218198991,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.9966178218198991\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"global_spearman\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.006089341254660887,\n        \"min\": 0.3433413437220531,\n        \"max\": 0.3541396785773762,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.3541396785773762\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["scaler = StandardScaler()\n","scaled_embeds = scaler.fit_transform(embeddings_l2)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1748455901862,"user_tz":-180,"elapsed":107,"user":{"displayName":"Милена","userId":"18103358405036649457"}},"id":"nB0QH_8fP0PB"},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["reducer = umap.UMAP(\n","    n_components=27,\n","    n_neighbors=10,\n","    min_dist=0.05,\n","    metric='correlation',\n","    random_state=42\n",")"],"metadata":{"executionInfo":{"status":"ok","timestamp":1748455901866,"user_tz":-180,"elapsed":2,"user":{"displayName":"Милена","userId":"18103358405036649457"}},"id":"iZ6Bg244P0PB"},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["umap_embeds = reducer.fit_transform(scaled_embeds)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1748455949155,"user_tz":-180,"elapsed":47286,"user":{"displayName":"Милена","userId":"18103358405036649457"}},"id":"DrUcvNSdP0PB"},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["#### Кластеризация контекстов"],"metadata":{"id":"U1koPd6epStI"}},{"cell_type":"code","source":["class MetaphorClusteringEnsemble:\n","    \"\"\"\n","    Консенсусная кластеризация для выявления метафорических употреблений глаголов\n","    \"\"\"\n","\n","    def __init__(self, data, algorithm_weights=None):\n","        \"\"\"\n","        Инициализация класса\n","\n","        Parameters:\n","        -----------\n","        data : array-like\n","            Матрица признаков (векторные представления употреблений)\n","        algorithm_weights : dict\n","            Веса для каждого алгоритма в консенсусной матрице\n","        \"\"\"\n","        self.data = np.array(data)\n","        self.n_samples = self.data.shape[0]\n","\n","        # Веса алгоритмов (HDBSCAN как самый надежный)\n","        self.weights = algorithm_weights or {\n","            'hdbscan': 0.5,\n","            'dbscan': 0.3,\n","            'spectral': 0.2\n","        }\n","\n","        self.best_params = {}\n","        self.clusterings = {}\n","        self.consensus_matrix = None\n","        self.final_clusters = None\n","\n","    def optimize_dbscan(self, eps_range=None, min_samples_range=None):\n","        \"\"\"\n","        Подбор оптимальных параметров для DBSCAN\n","        \"\"\"\n","        print(\"Оптимизация параметров DBSCAN...\")\n","\n","        if eps_range is None:\n","            # Автоматическое определение диапазона eps на основе k-distance\n","            k = 4  # минимальное количество точек для формирования кластера\n","            nbrs = NearestNeighbors(n_neighbors=k).fit(self.data)\n","            distances, indices = nbrs.kneighbors(self.data)\n","            distances = np.sort(distances[:, k-1], axis=0)\n","\n","            # Используем метод \"локтя\" для определения оптимального eps\n","            knee_point = self._find_knee_point(distances)\n","            eps_range = np.linspace(distances[knee_point] * 0.5, distances[knee_point] * 2, 20)\n","\n","        if min_samples_range is None:\n","            min_samples_range = range(3, min(20, self.n_samples // 10))\n","\n","        best_score = -1\n","        best_params = {}\n","\n","        param_grid = ParameterGrid({\n","            'eps': eps_range,\n","            'min_samples': min_samples_range\n","        })\n","\n","        for params in param_grid:\n","            try:\n","                dbscan = DBSCAN(**params)\n","                labels = dbscan.fit_predict(self.data)\n","\n","                # Проверяем, что есть хотя бы 2 кластера (исключая шум)\n","                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n","                if n_clusters < 2:\n","                    continue\n","\n","                # Оценка качества (без учета точек шума для silhouette)\n","                mask = labels != -1\n","                if np.sum(mask) < 2:\n","                    continue\n","\n","                score = silhouette_score(self.data[mask], labels[mask])\n","\n","                if score > best_score:\n","                    best_score = score\n","                    best_params = params.copy()\n","\n","            except:\n","                continue\n","\n","        self.best_params['dbscan'] = best_params\n","        print(f\"Лучшие параметры DBSCAN: {best_params}, Score: {best_score:.3f}\")\n","\n","    def optimize_hdbscan(self, min_cluster_size_range=None, min_samples_range=None):\n","        \"\"\"\n","        Подбор оптимальных параметров для HDBSCAN\n","        \"\"\"\n","        print(\"Оптимизация параметров HDBSCAN...\")\n","\n","        if min_cluster_size_range is None:\n","            min_cluster_size_range = np.arange(3, 50, 7)\n","\n","        if min_samples_range is None:\n","            min_samples_range = np.arange(1, 20, 5)\n","\n","        best_score = -1\n","        best_params = {}\n","\n","        param_grid = ParameterGrid({\n","            'min_cluster_size': min_cluster_size_range,\n","            'min_samples': min_samples_range,\n","            'cluster_selection_epsilon': [0.0, 0.1, 0.2, 0.5]\n","        })\n","\n","        for params in param_grid:\n","            try:\n","                clusterer = hdbscan.HDBSCAN(**params)\n","                labels = clusterer.fit_predict(self.data)\n","\n","                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n","                if n_clusters < 2:\n","                    continue\n","\n","                mask = labels != -1\n","                if np.sum(mask) < 2:\n","                    continue\n","\n","                # Комбинированная метрика: silhouette + DBCV (если доступно)\n","                silhouette = silhouette_score(self.data[mask], labels[mask])\n","\n","                # HDBSCAN предоставляет собственную метрику качества\n","                try:\n","                    dbcv_score = clusterer.relative_validity_\n","                    if dbcv_score is not None:\n","                        score = 0.4 * silhouette + 0.6 * dbcv_score\n","                    else:\n","                        score = silhouette\n","                except:\n","                    score = silhouette\n","\n","                if score > best_score:\n","                    best_score = score\n","                    best_params = params.copy()\n","\n","            except:\n","                continue\n","\n","        self.best_params['hdbscan'] = best_params\n","        print(f\"Лучшие параметры HDBSCAN: {best_params}, Score: {best_score:.3f}\")\n","\n","    def optimize_spectral(self, max_clusters=None):\n","        \"\"\"\n","        Подбор оптимального количества кластеров для Spectral Clustering\n","        \"\"\"\n","        print(\"Оптимизация параметров Spectral Clustering...\")\n","\n","        if max_clusters is None:\n","            max_clusters = min(20, self.n_samples // 3)\n","\n","        # Анализ собственных значений лапласиана для определения количества кластеров\n","        similarity_matrix = self._compute_similarity_matrix()\n","        L = laplacian(similarity_matrix, normed=True)\n","        eigenvals = np.linalg.eigvals(L.toarray() if hasattr(L, 'toarray') else L)\n","        eigenvals = np.sort(eigenvals)\n","\n","        # Поиск наибольшего разрыва в собственных значениях\n","        gaps = np.diff(eigenvals[:max_clusters])\n","        spectral_suggestion = np.argmax(gaps) + 2\n","\n","        best_score = -1\n","        best_params = {}\n","\n","        # Тестируем диапазон кластеров вокруг предложенного спектральным анализом\n","        k_range = range(max(2, spectral_suggestion - 3),\n","                       min(max_clusters + 1, spectral_suggestion + 5))\n","\n","        for n_clusters in k_range:\n","            try:\n","                for affinity in ['rbf', 'nearest_neighbors']:\n","                    spectral = SpectralClustering(\n","                        n_clusters=n_clusters,\n","                        affinity=affinity,\n","                        random_state=42\n","                    )\n","                    labels = spectral.fit_predict(self.data)\n","\n","                    score = silhouette_score(self.data, labels)\n","\n","                    if score > best_score:\n","                        best_score = score\n","                        best_params = {\n","                            'n_clusters': n_clusters,\n","                            'affinity': affinity\n","                        }\n","\n","            except:\n","                continue\n","\n","        self.best_params['spectral'] = best_params\n","        print(f\"Лучшие параметры Spectral: {best_params}, Score: {best_score:.3f}\")\n","\n","    def _compute_similarity_matrix(self):\n","        \"\"\"Вычисление матрицы сходства для спектральной кластеризации\"\"\"\n","        distances = pdist(self.data, metric='euclidean')\n","        distance_matrix = squareform(distances)\n","\n","        # RBF kernel\n","        gamma = 1.0 / (2 * np.var(distances))\n","        similarity_matrix = np.exp(-gamma * distance_matrix ** 2)\n","\n","        return similarity_matrix\n","\n","    def _find_knee_point(self, values):\n","        \"\"\"Поиск точки 'колена' в отсортированном массиве\"\"\"\n","        n_points = len(values)\n","        all_coords = np.vstack((range(n_points), values)).T\n","\n","        # Первая и последняя точки\n","        first_point = all_coords[0]\n","        last_point = all_coords[-1]\n","\n","        # Вектор от первой к последней точке\n","        line_vec = last_point - first_point\n","        line_vec_norm = line_vec / np.sqrt(np.sum(line_vec**2))\n","\n","        # Векторы от первой точки к каждой точке\n","        vec_from_first = all_coords - first_point\n","\n","        # Проекции на линию\n","        scalar_proj = np.dot(vec_from_first, line_vec_norm.reshape(-1, 1)).flatten()\n","        vec_proj = np.outer(scalar_proj, line_vec_norm)\n","\n","        # Перпендикулярные расстояния\n","        distances = np.sqrt(np.sum((vec_from_first - vec_proj)**2, axis=1))\n","\n","        return np.argmax(distances)\n","\n","    def fit_all_algorithms(self):\n","        \"\"\"Обучение всех алгоритмов с оптимальными параметрами\"\"\"\n","        print(\"\\nОбучение алгоритмов кластеризации...\")\n","\n","        # DBSCAN\n","        if 'dbscan' in self.best_params:\n","            dbscan = DBSCAN(**self.best_params['dbscan'])\n","            self.clusterings['dbscan'] = dbscan.fit_predict(self.data)\n","            print(f\"DBSCAN: {len(set(self.clusterings['dbscan'])) - (1 if -1 in self.clusterings['dbscan'] else 0)} кластеров\")\n","\n","        # HDBSCAN\n","        if 'hdbscan' in self.best_params:\n","            hdbscan_clusterer = hdbscan.HDBSCAN(**self.best_params['hdbscan'])\n","            self.clusterings['hdbscan'] = hdbscan_clusterer.fit_predict(self.data)\n","            print(f\"HDBSCAN: {len(set(self.clusterings['hdbscan'])) - (1 if -1 in self.clusterings['hdbscan'] else 0)} кластеров\")\n","\n","        # Spectral Clustering\n","        if 'spectral' in self.best_params:\n","            spectral = SpectralClustering(**self.best_params['spectral'], random_state=42)\n","            self.clusterings['spectral'] = spectral.fit_predict(self.data)\n","            print(f\"Spectral: {self.best_params['spectral']['n_clusters']} кластеров\")\n","\n","    def compute_consensus_matrix(self):\n","        \"\"\"Вычисление взвешенной консенсусной матрицы\"\"\"\n","        print(\"\\nВычисление консенсусной матрицы...\")\n","\n","        self.consensus_matrix = np.zeros((self.n_samples, self.n_samples))\n","        total_weight = 0\n","\n","        for alg_name, labels in self.clusterings.items():\n","            if alg_name not in self.weights:\n","                continue\n","\n","            weight = self.weights[alg_name]\n","            total_weight += weight\n","\n","            # Создание матрицы совместного вхождения для текущего алгоритма\n","            cooccurrence_matrix = np.zeros((self.n_samples, self.n_samples))\n","\n","            for i in range(self.n_samples):\n","                for j in range(i, self.n_samples):\n","                    # Проверяем, что обе точки не являются шумом (-1)\n","                    if labels[i] != -1 and labels[j] != -1 and labels[i] == labels[j]:\n","                        cooccurrence_matrix[i, j] = 1\n","                        cooccurrence_matrix[j, i] = 1\n","\n","            self.consensus_matrix += weight * cooccurrence_matrix\n","\n","        # Нормализация на общий вес\n","        self.consensus_matrix /= total_weight\n","\n","        print(\"Консенсусная матрица вычислена\")\n","\n","    def extract_final_clusters(self, threshold=0.5):\n","        \"\"\"\n","        Извлечение финальных консенсусных кластеров\n","\n","        Parameters:\n","        -----------\n","        threshold : float\n","            Порог для определения принадлежности к одному кластеру\n","        \"\"\"\n","        print(f\"\\nИзвлечение финальных кластеров (порог: {threshold})...\")\n","\n","        # Создание графа связей на основе консенсусной матрицы\n","        adjacency_matrix = (self.consensus_matrix >= threshold).astype(int)\n","\n","        # Поиск связанных компонент (кластеров)\n","        visited = np.zeros(self.n_samples, dtype=bool)\n","        clusters = []\n","        cluster_id = 0\n","        self.final_clusters = np.full(self.n_samples, -1)  # -1 для неназначенных точек\n","\n","        def dfs(node, current_cluster):\n","            \"\"\"Поиск в глубину для нахождения связанных компонент\"\"\"\n","            visited[node] = True\n","            current_cluster.append(node)\n","\n","            for neighbor in range(self.n_samples):\n","                if adjacency_matrix[node, neighbor] and not visited[neighbor]:\n","                    dfs(neighbor, current_cluster)\n","\n","        for i in range(self.n_samples):\n","            if not visited[i]:\n","                current_cluster = []\n","                dfs(i, current_cluster)\n","\n","                # Сохраняем только кластеры с более чем одной точкой\n","                if len(current_cluster) > 1:\n","                    clusters.append(current_cluster)\n","                    for point in current_cluster:\n","                        self.final_clusters[point] = cluster_id\n","                    cluster_id += 1\n","\n","        n_clusters = len(clusters)\n","        n_noise = np.sum(self.final_clusters == -1)\n","\n","        print(f\"Найдено {n_clusters} консенсусных кластеров\")\n","        print(f\"Точек шума: {n_noise}\")\n","\n","        # Статистика по размерам кластеров\n","        if n_clusters > 0:\n","            cluster_sizes = [len(cluster) for cluster in clusters]\n","            print(f\"Размеры кластеров: мин={min(cluster_sizes)}, макс={max(cluster_sizes)}, средний={np.mean(cluster_sizes):.1f}\")\n","\n","        return self.final_clusters\n","\n","    def fit(self, dbscan_params=None, hdbscan_params=None, spectral_params=None,\n","            consensus_threshold=0.5):\n","        \"\"\"\n","        Полный пайплайн кластеризации\n","\n","        Parameters:\n","        -----------\n","        dbscan_params, hdbscan_params, spectral_params : dict\n","            Параметры для оптимизации каждого алгоритма\n","        consensus_threshold : float\n","            Порог для извлечения финальных кластеров\n","        \"\"\"\n","        print(\"=== ЗАПУСК КОНСЕНСУСНОЙ КЛАСТЕРИЗАЦИИ ===\")\n","\n","        # Оптимизация параметров\n","        self.optimize_dbscan(**(dbscan_params or {}))\n","        self.optimize_hdbscan(**(hdbscan_params or {}))\n","        self.optimize_spectral(**(spectral_params or {}))\n","\n","        # Обучение алгоритмов\n","        self.fit_all_algorithms()\n","\n","        # Построение консенсуса\n","        self.compute_consensus_matrix()\n","\n","        # Извлечение финальных кластеров\n","        final_clusters = self.extract_final_clusters(threshold=consensus_threshold)\n","\n","        return final_clusters"],"metadata":{"executionInfo":{"status":"ok","timestamp":1748455949165,"user_tz":-180,"elapsed":4,"user":{"displayName":"Милена","userId":"18103358405036649457"}},"id":"i6WMdzB8pStP"},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["def visualize_consensus_clusters(data, clusters, method='both', figsize=(16, 7),\n","                               save_path=None, dpi=300):\n","    \"\"\"\n","    Визуализация результатов консенсусной кластеризации\n","\n","    Parameters:\n","    -----------\n","    data : np.ndarray\n","        Исходные данные (векторные представления)\n","    clusters : np.ndarray\n","        Метки кластеров (-1 для шума)\n","    method : str\n","        Метод снижения размерности: 'pca', 'tsne', или 'both'\n","    figsize : tuple\n","        Размер фигуры\n","    save_path : str\n","        Путь для сохранения графика\n","    dpi : int\n","        Разрешение для сохранения\n","    \"\"\"\n","\n","    # Настройка стиля и цветов\n","    sns.set(style=\"whitegrid\", font_scale=1.0)\n","    palette = sns.color_palette(\"husl\", 8)\n","    ITALIAN_COLOR = palette[0]\n","\n","    # Получаем уникальные кластеры (исключаем шум -1)\n","    unique_clusters = np.unique(clusters)\n","    noise_mask = clusters == -1\n","    n_clusters = len(unique_clusters) - (1 if -1 in unique_clusters else 0)\n","\n","    # Создаем цветовую палитру\n","    cluster_colors = sns.color_palette(\"husl\", n_colors=max(n_clusters, 8))\n","    if n_clusters > 0:\n","        cluster_colors[0] = ITALIAN_COLOR  # Первый кластер всегда итальянский\n","\n","    # Настройка параметров визуализации\n","    POINT_ALPHA = 0.85\n","    NOISE_ALPHA = 0.5\n","    BORDER_ALPHA = 0.9\n","    ANNOTATION_FONT = {'size': 12, 'weight': 'semibold', 'color': 'white'}\n","\n","    # Создаем фигуру\n","    if method == 'both':\n","        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize, facecolor='white')\n","        axes = [ax1, ax2]\n","        methods = ['PCA', 't-SNE']\n","    else:\n","        fig, ax = plt.subplots(figsize=(figsize[0]//2, figsize[1]), facecolor='white')\n","        axes = [ax]\n","        methods = ['PCA' if method == 'pca' else 't-SNE']\n","\n","    # Применяем методы снижения размерности\n","    reducers = []\n","    if 'PCA' in methods:\n","        pca = PCA(n_components=2, random_state=42)\n","        data_pca = pca.fit_transform(data)\n","        reducers.append(('PCA', data_pca, pca))\n","\n","    if 't-SNE' in methods:\n","        tsne = TSNE(n_components=2, random_state=42,\n","                   perplexity=min(30, len(data)//4), init='pca')\n","        data_tsne = tsne.fit_transform(data)\n","        reducers.append(('t-SNE', data_tsne, None))\n","\n","    # Создаем визуализации\n","    for idx, (reduction_name, data_2d, reducer) in enumerate(reducers):\n","        ax = axes[idx]\n","        ax.set_facecolor('white')\n","\n","        # Рисуем шум (если есть)\n","        if np.any(noise_mask):\n","            ax.scatter(data_2d[noise_mask, 0], data_2d[noise_mask, 1],\n","                      c='#d9d9d9', alpha=NOISE_ALPHA, s=25,\n","                      marker='.', edgecolors='none',\n","                      label='Шум')\n","\n","        # Визуализация кластеров\n","        cluster_counter = 1  # Начинаем нумерацию с 1\n","        for cluster_id in unique_clusters:\n","            if cluster_id == -1: continue\n","\n","            mask = clusters == cluster_id\n","            cluster_points = data_2d[mask]\n","            color = cluster_colors[(cluster_counter-1) % len(cluster_colors)]\n","\n","            # Основные точки кластера\n","            ax.scatter(cluster_points[:, 0], cluster_points[:, 1],\n","                     c=[color], alpha=POINT_ALPHA, s=65,\n","                     edgecolors='w', linewidth=0.6,\n","                     label=f'Кластер {cluster_counter}', zorder=3)\n","\n","            # Границы кластера\n","            if len(cluster_points) >= 3:\n","                try:\n","                    hull = ConvexHull(cluster_points)\n","                    hull_points = cluster_points[hull.vertices]\n","\n","                    # Сглаженные границы\n","                    poly = mpatches.Polygon(hull_points, closed=True,\n","                                          fill=False, edgecolor=color,\n","                                          linewidth=1.8, linestyle='-',\n","                                          alpha=BORDER_ALPHA, zorder=2)\n","                    ax.add_patch(poly)\n","                except:\n","                    if len(cluster_points) >= 2:\n","                        cov = np.cov(cluster_points.T)\n","                        vals, vecs = np.linalg.eigh(cov)\n","                        angle = np.degrees(np.arctan2(*vecs[:, 0][::-1]))\n","                        w, h = 3 * np.sqrt(vals)\n","\n","                        ellipse = Ellipse(cluster_points.mean(0), w, h, angle,\n","                                         fill=False, edgecolor=color,\n","                                         linewidth=1.8, alpha=BORDER_ALPHA)\n","                        ax.add_patch(ellipse)\n","\n","            # Аннотация кластера\n","            center = cluster_points.mean(axis=0)\n","            ax.text(center[0], center[1], str(cluster_counter),\n","                   ha='center', va='center', **ANNOTATION_FONT,\n","                   bbox=dict(boxstyle=\"circle,pad=0.3\",\n","                            facecolor=color, alpha=0.9,\n","                            edgecolor='none'))\n","\n","            cluster_counter += 1\n","\n","        # Заголовки и подзаголовки\n","        ax.set_title(f'Консенсусные кластеры\\n(Метод проекции: {reduction_name})',\n","                    fontsize=13, pad=12)\n","\n","        # Оформление осей\n","        ax.set_xlabel(f'{reduction_name} 1', labelpad=10)\n","        ax.set_ylabel(f'{reduction_name} 2', labelpad=10)\n","        ax.tick_params(axis='both', which='both', length=0)\n","\n","        # Добавление легенды\n","        if idx == 0:\n","            handles = [plt.Line2D([0], [0], marker='o', color='w',\n","                      markerfacecolor=c, markersize=10, alpha=0.8)\n","                     for i, c in enumerate(cluster_colors) if i < n_clusters]\n","            labels = [f'Кластер {i+1}' for i in range(n_clusters)]\n","            if np.any(noise_mask):\n","                handles.append(plt.Line2D([0], [0], marker='.', color='w',\n","                              markerfacecolor='#d9d9d9', markersize=10))\n","                labels.append('Шум')\n","\n","            legend = ax.legend(handles, labels, loc='upper right',\n","                             bbox_to_anchor=(1.15, 1.02),\n","                             frameon=True, framealpha=0.95,\n","                             borderpad=1)\n","            legend.get_frame().set_edgecolor('#333333')\n","\n","        # Объясненная дисперсия для PCA\n","        if reduction_name == 'PCA' and reducer is not None:\n","            explained_var = reducer.explained_variance_ratio_\n","            ax.text(0.03, 0.97,\n","                   f'Expl. Variance:\\nPC1: {explained_var[0]:.1%}\\nPC2: {explained_var[1]:.1%}',\n","                   transform=ax.transAxes, ha='left', va='top',\n","                   fontsize=10, bbox=dict(facecolor='white', alpha=0.8,\n","                                       edgecolor='none', pad=6))\n","\n","    # Общий заголовок\n","    if method == 'both':\n","        plt.suptitle('Результаты консенсусной кластеризации',\n","                   fontsize=16, y=1.02, weight='bold')\n","\n","    # Финализация\n","    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n","\n","    if save_path:\n","        plt.savefig(save_path, dpi=dpi, bbox_inches='tight',\n","                   facecolor='white', transparent=False)\n","        print(f\"График сохранён: {save_path}\")\n","\n","    plt.show()\n","\n","    # Вывод статистики\n","    print(\"\\n=== СТАТИСТИКА ===\")\n","    print(f\"Кластеров: {n_clusters}\")\n","    print(f\"Шум: {noise_mask.sum()} точек\")\n","    print(f\"Всего данных: {len(data)}\")\n","\n","    if n_clusters > 0:\n","        sizes = [np.sum(clusters == c) for c in unique_clusters if c != -1]\n","        print(f\"\\nРазмеры кластеров:\")\n","        print(pd.Series(sizes).describe().to_string())"],"metadata":{"id":"aRQXqI6ynMUi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def analyze_cluster_medoids(data, clusters, contexts=None, top_k=20,\n","                           distance_metric='euclidean', return_indices=False):\n","    \"\"\"\n","    Анализ медоидов кластеров и поиск ближайших к ним контекстов\n","\n","    Parameters:\n","    -----------\n","    data : np.ndarray\n","        Матрица признаков (векторные представления)\n","    clusters : np.ndarray\n","        Метки кластеров для каждой точки (-1 для шума)\n","    contexts : list or pd.Series, optional\n","        Список контекстов/предложений соответствующих каждой точке данных\n","    top_k : int, default=20\n","        Количество ближайших контекстов для вывода\n","    distance_metric : str, default='euclidean'\n","        Метрика расстояния для поиска медоида\n","    return_indices : bool, default=False\n","        Возвращать ли индексы медоидов и ближайших точек\n","\n","    Returns:\n","    --------\n","    dict : Словарь с результатами анализа для каждого кластера\n","    \"\"\"\n","\n","    # Проверяем входные данные\n","    if len(data) != len(clusters):\n","        raise ValueError(\"Размеры data и clusters должны совпадать\")\n","\n","    if contexts is not None and len(contexts) != len(data):\n","        raise ValueError(\"Размеры contexts и data должны совпадать\")\n","\n","    # Получаем уникальные кластеры (исключаем шум -1)\n","    unique_clusters = np.unique(clusters)\n","    valid_clusters = unique_clusters[unique_clusters != -1]\n","\n","    results = {}\n","\n","    print(\"=== АНАЛИЗ МЕДОИДОВ КЛАСТЕРОВ ===\\n\")\n","\n","    for cluster_id in valid_clusters:\n","        print(f\" КЛАСТЕР {cluster_id}\")\n","        print(\"=\" * 50)\n","\n","        # Получаем точки текущего кластера\n","        cluster_mask = clusters == cluster_id\n","        cluster_data = data[cluster_mask]\n","        cluster_indices = np.where(cluster_mask)[0]\n","        cluster_size = len(cluster_data)\n","\n","        print(f\"Размер кластера: {cluster_size} точек\")\n","\n","        # Находим медоид - точку с минимальной суммой расстояний до всех остальных в кластере\n","        if cluster_size == 1:\n","            medoid_idx_in_cluster = 0\n","            medoid_distance_sum = 0.0\n","        else:\n","            # Вычисляем матрицу расстояний внутри кластера\n","            intra_distances = pairwise_distances(cluster_data, metric=distance_metric)\n","\n","            # Находим точку с минимальной суммой расстояний (медоид)\n","            distance_sums = np.sum(intra_distances, axis=1)\n","            medoid_idx_in_cluster = np.argmin(distance_sums)\n","            medoid_distance_sum = distance_sums[medoid_idx_in_cluster]\n","\n","        # Индекс медоида в исходных данных\n","        medoid_global_idx = cluster_indices[medoid_idx_in_cluster]\n","        medoid_vector = data[medoid_global_idx]\n","\n","        print(f\"Медоид: точка #{medoid_global_idx}\")\n","        print(f\"Средняя внутрикластерная дистанция медоида: {medoid_distance_sum / cluster_size:.4f}\")\n","\n","        # Показываем контекст медоида, если есть\n","        if contexts is not None:\n","            print(f\"\\n КОНТЕКСТ МЕДОИДА:\")\n","            print(f\"   \\\"{contexts[medoid_global_idx]}\\\"\\n\")\n","\n","        # Находим top_k ближайших точек к медоиду из всего датасета\n","        distances_to_medoid = cdist([medoid_vector], data, metric=distance_metric)[0]\n","\n","        # Сортируем по расстоянию и берем top_k\n","        nearest_indices = np.argsort(distances_to_medoid)[:top_k]\n","        nearest_distances = distances_to_medoid[nearest_indices]\n","\n","        print(f\"ТОП-{top_k} БЛИЖАЙШИХ КОНТЕКСТОВ К МЕДОИДУ:\")\n","        print(\"-\" * 60)\n","\n","        # Сохраняем результаты для кластера\n","        cluster_results = {\n","            'medoid_index': medoid_global_idx,\n","            'medoid_vector': medoid_vector,\n","            'cluster_size': cluster_size,\n","            'avg_intra_distance': medoid_distance_sum / cluster_size,\n","            'nearest_indices': nearest_indices,\n","            'nearest_distances': nearest_distances\n","        }\n","\n","        if contexts is not None:\n","            cluster_results['medoid_context'] = contexts[medoid_global_idx]\n","            cluster_results['nearest_contexts'] = [contexts[idx] for idx in nearest_indices]\n","\n","            # Выводим ближайшие контексты\n","            for rank, (idx, dist) in enumerate(zip(nearest_indices, nearest_distances), 1):\n","                # Отмечаем, к какому кластеру принадлежит каждая точка\n","                point_cluster = clusters[idx]\n","                cluster_marker = f\"[К{point_cluster}]\" if point_cluster != -1 else \"[ШУМ]\"\n","\n","                # Особо выделяем медоид\n","                is_medoid = \"МЕДОИД\" if idx == medoid_global_idx else \"\"\n","\n","                print(f\"{rank:2d}. {cluster_marker} (дист: {dist:.4f}) {is_medoid}\")\n","                print(f\"    \\\"{contexts[idx]}\\\"\")\n","                print()\n","        else:\n","            # Если контекстов нет, просто показываем индексы и расстояния\n","            for rank, (idx, dist) in enumerate(zip(nearest_indices, nearest_distances), 1):\n","                point_cluster = clusters[idx]\n","                cluster_marker = f\"[К{point_cluster}]\" if point_cluster != -1 else \"[ШУМ]\"\n","                is_medoid = \"МЕДОИД\" if idx == medoid_global_idx else \"\"\n","\n","                print(f\"{rank:2d}. Точка #{idx} {cluster_marker} (дист: {dist:.4f}) {is_medoid}\")\n","\n","        results[cluster_id] = cluster_results\n","        print(\"\\n\" + \"=\"*80 + \"\\n\")\n","\n","    # Выводим общую статистику\n","    print(\"ОБЩАЯ СТАТИСТИКА ПО КЛАСТЕРАМ\")\n","    print(\"=\" * 50)\n","\n","    for cluster_id, result in results.items():\n","        print(f\"Кластер {cluster_id}: {result['cluster_size']} точек, \"\n","              f\"средняя дистанция до медоида: {result['avg_intra_distance']:.4f}\")\n","\n","    if return_indices:\n","        return results\n","    else:\n","        # Возвращаем упрощенную версию без векторов для удобства\n","        simple_results = {}\n","        for cluster_id, result in results.items():\n","            simple_results[cluster_id] = {\n","                'medoid_index': result['medoid_index'],\n","                'cluster_size': result['cluster_size'],\n","                'avg_intra_distance': result['avg_intra_distance'],\n","                'nearest_indices': result['nearest_indices'].tolist(),\n","                'nearest_distances': result['nearest_distances'].tolist()\n","            }\n","            if contexts is not None:\n","                simple_results[cluster_id]['medoid_context'] = result['medoid_context']\n","                simple_results[cluster_id]['nearest_contexts'] = result['nearest_contexts']\n","\n","        return simple_results\n","\n","def compare_medoids_similarity(results, data, distance_metric='euclidean'):\n","    \"\"\"\n","    Дополнительная функция для анализа схожести между медоидами разных кластеров\n","\n","    Parameters:\n","    -----------\n","    results : dict\n","        Результаты функции analyze_cluster_medoids с return_indices=True\n","    data : np.ndarray\n","        Исходная матрица данных\n","    distance_metric : str\n","        Метрика для вычисления расстояний\n","    \"\"\"\n","\n","    if len(results) < 2:\n","        print(\"Недостаточно кластеров для сравнения медоидов\")\n","        return\n","\n","    print(\"\\n МАТРИЦА РАССТОЯНИЙ МЕЖДУ МЕДОИДАМИ\")\n","    print(\"=\" * 60)\n","\n","    cluster_ids = list(results.keys())\n","    medoid_vectors = np.array([results[cid]['medoid_vector'] for cid in cluster_ids])\n","\n","    # Вычисляем матрицу расстояний между медоидами\n","    medoid_distances = pairwise_distances(medoid_vectors, metric=distance_metric)\n","\n","    # Создаем красивую таблицу\n","    df_distances = pd.DataFrame(medoid_distances,\n","                               index=[f\"К{cid}\" for cid in cluster_ids],\n","                               columns=[f\"К{cid}\" for cid in cluster_ids])\n","\n","    print(df_distances.round(4))\n","\n","    # Находим самые близкие и далекие пары\n","    mask = np.triu(np.ones_like(medoid_distances, dtype=bool), k=1)\n","    upper_distances = medoid_distances[mask]\n","\n","    if len(upper_distances) > 0:\n","        min_dist_idx = np.unravel_index(np.argmin(medoid_distances + np.eye(len(cluster_ids)) * 1000),\n","                                       medoid_distances.shape)\n","        max_dist_idx = np.unravel_index(np.argmax(medoid_distances), medoid_distances.shape)\n","\n","        print(f\"Самые близкие кластеры: К{cluster_ids[min_dist_idx[0]]} ↔ К{cluster_ids[min_dist_idx[1]]} \"\n","              f\"(расстояние: {medoid_distances[min_dist_idx]:.4f})\")\n","        print(f\"Самые далекие кластеры: К{cluster_ids[max_dist_idx[0]]} ↔ К{cluster_ids[max_dist_idx[1]]} \"\n","              f\"(расстояние: {medoid_distances[max_dist_idx]:.4f})\")"],"metadata":{"id":"x-xZWvOpnMUj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embeddings_list = umap_embeds.tolist()\n","\n","if len(embeddings_list) == len(en_featured_df):\n","    en_featured_df['embeddings'] = embeddings_list\n","else:\n","    print(\"Количество эмбеддингов не совпадает с количеством строк в DataFrame.\")"],"metadata":{"id":"OLdDufamUNsB","executionInfo":{"status":"ok","timestamp":1748455949210,"user_tz":-180,"elapsed":1,"user":{"displayName":"Милена","userId":"18103358405036649457"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["ensemble = MetaphorClusteringEnsemble(\n","    data=np.array(en_featured_df[en_featured_df['found_verbs'] == 'kick (пнуть)']['embeddings'].tolist()),\n","    algorithm_weights={\n","          'hdbscan': 0.5,  # Наибольший вес для HDBSCAN как самого надежного\n","          'dbscan': 0.2,\n","          'spectral': 0.3\n","      }\n",")\n","\n","final_clusters = ensemble.fit(consensus_threshold=0.4)"],"metadata":{"id":"CZItWpY6pStQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["visualize_consensus_clusters(\n","     data=np.array(en_featured_df[en_featured_df['found_verbs'] == 'kick (пнуть)']['embeddings'].tolist()),\n","     clusters=final_clusters,\n","     method='both'\n",")"],"metadata":{"id":"TMZLjlIW1TWU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["detailed_results = analyze_cluster_medoids(\n","    data=np.array(en_featured_df[en_featured_df['found_verbs'] == 'kick (пнуть)']['embeddings'].tolist()),\n","    clusters=final_clusters,\n","    contexts=np.array(en_featured_df[en_featured_df['found_verbs'] == 'kick (пнуть)']['context'].tolist()),\n","    return_indices=True\n",")\n","compare_medoids_similarity(detailed_results, np.array(en_featured_df[en_featured_df['found_verbs'] == 'kick (пнуть)']['embeddings'].tolist()))"],"metadata":{"id":"uV1kQfD_1TWV"},"execution_count":null,"outputs":[]}]}